{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will show different ways to work with videos using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this next line to confirm your camera device is attached.\n",
    "You should get output similar to: crw-rw---- 1 root video 81, 0 Jun 30 02:30 /dev/video0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: /dev/video*: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltrh /dev/video*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Camera errors:\n",
    "- Your camera is not connected\n",
    "- For remote containers, you did not add the parameter to attach your device. Stop the container and run again with the required parameter to attach your camera:\n",
    "-- E.G. sudo docker run --runtime nvidia -it --rm --network host ***--device /dev/video0***  dustynv/l4t-ml:r32.5.0-py3\n",
    "-If this is a windows machine, typically your primary camera will be device 0 (e.g.  cv2.VideoCapture(0) ) but if it is a USB camera and you have an embedded camera, it may be device 1. You can always set the value in the USBcamera class to cv.VideoCapture(1) and test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lm/kt3nd3_d5xbdlqg_f3xz91y40000gn/T/ipykernel_37467/2059199736.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cv' is not defined"
     ]
    }
   ],
   "source": [
    "cv.VideoCapture(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing camera feeds from local session\n",
    "This next session should ***not be using a remote headless connection and JupyterLab on the nano.***  \n",
    "It can be run locally if using a monitor and keyboard on Ubuntu or Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing camera feed in another window. Press any key to stop.\n"
     ]
    }
   ],
   "source": [
    "#Code adapted from book, Programming Computer Vision with Python\n",
    "#this code is just for reference only, do not run this on your nano, just your host machine\n",
    "import cv2\n",
    "\n",
    "clicked = False\n",
    "def onMouse(event, x, y, flags, param):\n",
    "    global clicked\n",
    "    if event == cv2.EVENT_LBUTTONUP:\n",
    "        clicked = True\n",
    "\n",
    "cameraCapture = cv2.VideoCapture(1)\n",
    "cv2.namedWindow('MyCamera')\n",
    "cv2.setMouseCallback('MyCamera', onMouse)\n",
    "\n",
    "print ('Showing camera feed in another window. Press any key to stop.')\n",
    "success, frame = cameraCapture.read()\n",
    "while cv2.waitKey(1) == -1 and not clicked:\n",
    "    if frame is not None:\n",
    "        cv2.imshow('MyCamera', frame)\n",
    "        #display(Image.fromarray(img3))\n",
    "    success, frame = cameraCapture.read()\n",
    "\n",
    "cv2.destroyWindow('MyCamera')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing camera feeds from your Jetson Nano\n",
    "On the Nano, you can work with video by using pyWidgets to show the videos within the JupyterLab notebook. For convenience, we are use the classes and functions jetcam utilities but I have included the classes in the first code block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jetcam utilities  https://github.com/NVIDIA-AI-IOT/jetcam\n",
    "#including classes here to reduce install when moving code between host machines and nano\n",
    "import traitlets\n",
    "import threading\n",
    "import numpy as np\n",
    "import cv2\n",
    "import atexit\n",
    "\n",
    "\n",
    "class Camera(traitlets.HasTraits):\n",
    "\n",
    "    value = traitlets.Any()\n",
    "    width = traitlets.Integer(default_value=224)\n",
    "    height = traitlets.Integer(default_value=224)\n",
    "    format = traitlets.Unicode(default_value='bgr8')\n",
    "    running = traitlets.Bool(default_value=False)\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Camera, self).__init__(*args, **kwargs)\n",
    "        if self.format == 'bgr8':\n",
    "            self.value = np.empty((self.height, self.width, 3), dtype=np.uint8)\n",
    "        self._running = False\n",
    "            \n",
    "    def _read(self):\n",
    "        \"\"\"Blocking call to read frame from camera\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def read(self):\n",
    "        if self._running:\n",
    "            raise RuntimeError('Cannot read directly while camera is running')\n",
    "        self.value = self._read()\n",
    "        return self.value\n",
    "    \n",
    "    def _capture_frames(self):\n",
    "        while True:\n",
    "            if not self._running:\n",
    "                break\n",
    "            self.value = self._read()\n",
    "            \n",
    "    @traitlets.observe('running')\n",
    "    def _on_running(self, change):\n",
    "        if change['new'] and not change['old']:\n",
    "            # transition from not running -> running\n",
    "            self._running = True\n",
    "            self.thread = threading.Thread(target=self._capture_frames)\n",
    "            self.thread.start()\n",
    "        elif change['old'] and not change['new']:\n",
    "            # transition from running -> not running\n",
    "            self._running = False\n",
    "            self.thread.join()\n",
    "\n",
    "\n",
    "def bgr8_to_jpeg(value, quality=75):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])\n",
    "\n",
    "class USBCamera(Camera):\n",
    "    \n",
    "    capture_fps = traitlets.Integer(default_value=30)\n",
    "    capture_width = traitlets.Integer(default_value=640)\n",
    "    capture_height = traitlets.Integer(default_value=480)   \n",
    "    capture_device = traitlets.Integer(default_value=0)\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(USBCamera, self).__init__(*args, **kwargs)\n",
    "        try:\n",
    "            #self.cap = cv2.VideoCapture(self._gst_str(), cv2.CAP_GSTREAMER)\n",
    "            self.cap = cv2.VideoCapture(0) #if on nano, change to 0\n",
    "\n",
    "            re , image = self.cap.read()\n",
    "            \n",
    "            if not re:\n",
    "                raise RuntimeError('Could not read image from camera.')\n",
    "            \n",
    "        except:\n",
    "            raise RuntimeError(\n",
    "                'Could not initialize camera.  Please see error trace.')\n",
    "\n",
    "        atexit.register(self.cap.release)\n",
    "                \n",
    "    def _gst_str(self):\n",
    "        return 'v4l2src device=/dev/video{} ! video/x-raw, width=(int){}, height=(int){}, framerate=(fraction){}/1 ! videoconvert !  video/x-raw, format=(string)BGR ! appsink'.format(self.capture_device, self.capture_width, self.capture_height, self.capture_fps)\n",
    "    \n",
    "    def _read(self):\n",
    "        re, image = self.cap.read()\n",
    "        if re:\n",
    "            image_resized = cv2.resize(image,(int(self.width),int(self.height)))\n",
    "            return image_resized\n",
    "        else:\n",
    "            raise RuntimeError('Could not read image from camera')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adapted from the Hello Camera lab from Getting Started with AI on the Jetson\n",
    "#from jetcam.usb_camera import USBCamera\n",
    "\n",
    "#TODO change capture_device if incorrect for your system\n",
    "camera = USBCamera(width=224, height=224, capture_width=640, capture_height=480, capture_device=0)\n",
    "\n",
    "image = camera.read()\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "type(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use Ipywidgets to display the image captured from the camera feed\n",
    "this code is adapted from: https://github.com/QuantStack/quantstack-talks/blob/master/2018-11-14-PyParis-widgets/notebooks/1.ipywidgets.ipynb\n",
    "\n",
    "This first code block will create a widget and update it with one image from the camera.\n",
    "\n",
    "If the widget does not display but you do not get an error message, just a binary value then you most likely need to install node.js and ipywidgets for JupyerLab, bind the ipwidgets and then refresh or restart your JupyterLab session.  Follow the instructions here for your host machine: https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-in-jupyterlab-3-0.\n",
    "\n",
    "If you are experiencing this on the nano, you should make sure you have the latest image and container recommended by your instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ec752b58be4914b399cb1f93ea9ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "#import bgr8_to_jpeg\n",
    "\n",
    "image_widget = ipywidgets.Image(format='jpeg')\n",
    "\n",
    "#this is using the image defined in the previous code block that is reading the camera stream\n",
    "#reading one image\n",
    "image_widget.value = bytes(cv2.imencode('.jpg', image)[1])\n",
    "# image_widget.value = bgr8_to_jpeg(image)\n",
    "\n",
    "display(image_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display the full video stream, we need to update the camera feed for video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.running = True\n",
    "\n",
    "def update_image(change):\n",
    "    image = change['new']\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    \n",
    "camera.observe(update_image, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the video feed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve(update_image, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the images from the camera feed to gray - CPU\n",
    "Lets first try using OpenCV without using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets transform the image\n",
    "camera.running = True\n",
    "\n",
    "def update_image(change):\n",
    "    image = change['new']\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    image_widget.value = bgr8_to_jpeg(gray)\n",
    "    \n",
    "camera.observe(update_image, names='value')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the images from the camera feed to gray - GPU\n",
    "Now let's use OpenCV CUDA\n",
    "Note the extra steps required to first upload the images to the GPU, process them and then download the images back to the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/Anaconda/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Applications/Anaconda/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/lm/kt3nd3_d5xbdlqg_f3xz91y40000gn/T/ipykernel_38432/1284430945.py\", line 38, in _capture_frames\n",
      "  File \"/Applications/Anaconda/anaconda3/lib/python3.8/site-packages/traitlets/traitlets.py\", line 606, in __set__\n",
      "    self.set(obj, value)\n",
      "  File \"/Applications/Anaconda/anaconda3/lib/python3.8/site-packages/traitlets/traitlets.py\", line 595, in set\n",
      "    obj._notify_trait(self.name, old_value, new_value)\n",
      "  File \"/Applications/Anaconda/anaconda3/lib/python3.8/site-packages/traitlets/traitlets.py\", line 1219, in _notify_trait\n",
      "    self.notify_change(Bunch(\n",
      "  File \"/Applications/Anaconda/anaconda3/lib/python3.8/site-packages/traitlets/traitlets.py\", line 1229, in notify_change\n",
      "    return self._notify_observers(change)\n",
      "  File \"/Applications/Anaconda/anaconda3/lib/python3.8/site-packages/traitlets/traitlets.py\", line 1266, in _notify_observers\n",
      "    c(event)\n",
      "  File \"/var/folders/lm/kt3nd3_d5xbdlqg_f3xz91y40000gn/T/ipykernel_38432/1368040652.py\", line 7, in update_image\n",
      "cv2.error: OpenCV(4.6.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/core/include/opencv2/core/private.cuda.hpp:106: error: (-216:No CUDA support) The library is compiled without CUDA support in function 'throw_no_cuda'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lets transform the image the CUDA way\n",
    "camera.running = True\n",
    "\n",
    "def update_image(change):\n",
    "    image = change['new']\n",
    "    gpu_frame = cv2.cuda_GpuMat()\n",
    "    gpu_frame.upload(image)\n",
    "    gray = cv2.cuda.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = gray.download()\n",
    "    image_widget.value = bgr8_to_jpeg(gray)\n",
    "    \n",
    "camera.observe(update_image, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve(update_image, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we are going to directly read in a video file\n",
    "We will again use the Video library of ipwidgets rather than the jetcam utilities.  \n",
    "The video in this example should be an MP4 video. \n",
    "The code below will create a widget and play the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf47698c0ff145508778f3430cb25a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00\\x14ftypmp42\\x00\\x00\\x02\\x00mp42\\x00\\x00\\x00\\x08free\\x00\\x1b50mdat\\x00\\x00\\x00\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Video, Image\n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "video = Video.from_file('hallway.mp4')\n",
    "video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a cv2 Canny transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264_videotoolbox @ 0x7fd28b840200] Error: cannot create compression session: -12903\n",
      "[h264_videotoolbox @ 0x7fd28b840200] Try -allow_sw 1. The hardware encoder may be busy, or not supported.\n",
      "[ERROR:0@15628.151] global /Users/runner/work/opencv-python/opencv-python/opencv/modules/videoio/src/cap_ffmpeg_impl.hpp (2985) open Could not open codec h264_videotoolbox, error: Unspecified error (-542398533)\n",
      "[ERROR:0@15628.151] global /Users/runner/work/opencv-python/opencv-python/opencv/modules/videoio/src/cap_ffmpeg_impl.hpp (3002) open VIDEOIO/FFMPEG: Failed to initialize VideoWriter\n",
      "2023-01-30 09:36:49.404 python[38432:4083742] AVF: AVAssetWriter status: Cannot Save\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture('hallway.mp4')\n",
    "\n",
    "frames = []\n",
    "\n",
    "while(1):\n",
    "    try:\n",
    "        _, frame = cap.read()\n",
    "\n",
    "        #fgmask = cv2.Canny(frame, 100, 100)\n",
    "        fgmask = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        mask = fgmask > 100\n",
    "        frame[mask, :] = 0\n",
    "\n",
    "        frames.append(frame)\n",
    "    except Exception:\n",
    "        break\n",
    "\n",
    "width = int(cap.get(3))\n",
    "height = int(cap.get(4))\n",
    "\n",
    "filename = 'output.mp4'\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "writer = cv2.VideoWriter(filename, fourcc, 25, (width, height))\n",
    "\n",
    "for frame in frames:\n",
    "    writer.write(frame)\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "    video.value = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lm/kt3nd3_d5xbdlqg_f3xz91y40000gn/T/ipykernel_38432/1758961975.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpycuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpycuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSourceModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycuda'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Aug 13 12:15:39 2018\n",
    "\n",
    "@author: bhaumik\n",
    "\"\"\"\n",
    "\n",
    "import pycuda.driver as drv\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pycuda.autoinit\n",
    "\n",
    "mod = SourceModule \\\n",
    "    (\n",
    "        \"\"\"\n",
    "#include<stdio.h>\n",
    "#define INDEX(a, b) a*256+b\n",
    "\n",
    "__global__ void bgr2gray(float *d_result,float *b_img, float *g_img, float *r_img)\n",
    "{\n",
    "unsigned int idx = threadIdx.x+(blockIdx.x*(blockDim.x*blockDim.y));\n",
    "\n",
    "unsigned int a = idx/256;\n",
    "unsigned int b = idx%256;\n",
    "d_result[INDEX(a, b)] = (0.299*r_img[INDEX(a, b)]+0.587*g_img[INDEX(a, b)]+0.114*b_img[INDEX(a, b)]);\n",
    "\n",
    "}\n",
    "  \n",
    "  \"\"\"\n",
    "      )\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "bgr2gray = mod.get_function(\"bgr2gray\")\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, h_img = cap.read()\n",
    "    h_img = cv2.resize(h_img,(256,256),interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "    b_img = h_img[:, :, 0].reshape(65536).astype(np.float32)\n",
    "    g_img = h_img[:, :, 1].reshape(65536).astype(np.float32)\n",
    "    r_img = h_img[:, :, 2].reshape(65536).astype(np.float32)\n",
    "    h_result=r_img\n",
    "   \n",
    "    bgr2gray(drv.Out(h_result), drv.In(b_img), drv.In(g_img),drv.In(r_img),block=(1024, 1, 1), grid=(64, 1, 1))\n",
    "\n",
    "    h_result=np.reshape(h_result,(256,256)).astype(np.uint8)\n",
    "    cv2.imshow(\"Grayscale Image\",h_result)\n",
    "    \n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Original frame',h_img)\n",
    "    if cv2.waitKey(50) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next code block will close any open widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Widget\n",
    "Widget.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next code block is a good idea to run before you move to another notebook to both release your camera if you ran the part of the code that uses your camera device and release available compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
