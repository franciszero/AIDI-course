{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2JZt1GL5D6W"
   },
   "source": [
    "# **Introduction to CUDA and PyCUDA**\n",
    "\n",
    "PyCUDA gives you easy, Pythonic access to Nvidia’s CUDA parallel computation API. \n",
    "\n",
    "*   Abstractions make CUDA programming easier\n",
    "*   Full power of CUDA API if needed\n",
    "*   Automatic error checking and cleanup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rakk0eUMHRR7"
   },
   "source": [
    "## **Initialization**\n",
    "A few modules have to be loaded to initialize communication to the GPU:\n",
    "\n",
    "*   import pycuda.driver as cuda\n",
    "*   import pycuda.autoinit\n",
    "\n",
    "The pycuda.driver module has methods that:\n",
    "1. Allocate memory on the GPU (cuda.mem alloc())\n",
    "2. Copy numpy arrays to the GPU (cuda.memcpy htod())\n",
    "3. Execute kernels on the GPU described by CUDA code (see compiler.SourceModule)\n",
    "4. Copy data from the GPU back to numpy arrays (cuda.memcpy dtoh())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FA_YN7HlGRP5"
   },
   "outputs": [],
   "source": [
    "# Import PyCUDA and several modules associated with the PyCUDA\n",
    "!pip install pycuda \n",
    "import pycuda\n",
    "import pycuda.driver as cuda\n",
    "cuda.init()\n",
    "\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda.gpuarray as gpuarray\n",
    "from pycuda.curandom import rand as curand\n",
    "from pycuda.elementwise import ElementwiseKernel\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7uA7OVrTLIl"
   },
   "source": [
    "## **CUDA device query**\n",
    "\n",
    "A GPU query is a very basic operation that will tell us the specific technical details of our GPU, such as available GPU memory and core count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtnIrmDaQmm9"
   },
   "outputs": [],
   "source": [
    "print('CUDA device query (PyCUDA version) \\n')\n",
    "print('Detected {} CUDA Capable device(s) \\n'.format(cuda.Device.count()))\n",
    "for i in range(cuda.Device.count()):\n",
    "    \n",
    "    gpu_device = cuda.Device(i)\n",
    "    print('Device {}: {}'.format( i, gpu_device.name() ) )\n",
    "    compute_capability = float( '%d.%d' % gpu_device.compute_capability() )\n",
    "    print('\\t Compute Capability: {}'.format(compute_capability))\n",
    "    print('\\t Total Memory: {} megabytes'.format(gpu_device.total_memory()//(1024**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cE8Wg9b84Zvi"
   },
   "source": [
    "## **Basics of GPU programming**\n",
    "\n",
    "We'll see how to write and read data to and from the GPU's memory, and how to write some very simple elementwise GPU functions in CUDA C.\n",
    "\n",
    "### PyCUDA's gpuarray class\n",
    "\n",
    "PyCUDA's gpuarray class has important role within GPU programming in Python. This has all of the features from NumPy—multidimensional vector/matrix/tensor shape structuring, array-slicing, array unraveling, and overloaded operators for point-wise computations (for example, +, -, *, /, and **).\n",
    "\n",
    "### Transfering data to and from the GPU with gpuarray\n",
    "\n",
    "GPU has its own memory apart from the host computer's memory, which is known as device memory. \n",
    "In CUDA C, data is transfferd back and forth between the CPU to the GPU (with commands such as cudaMemcpyHostToDevice and cudaMemcpyDeviceToHost).\n",
    "\n",
    "Fortunately, PyCUDA covers all of the overhead of memory allocation, deallocation, and data transfers with the gpuarray class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0h99JVJn6T2"
   },
   "outputs": [],
   "source": [
    "#contain the host data\n",
    "host_data = np.array([1,2,3,4,5], dtype=np.float32)\n",
    "#transfer the host data to the GPU and create a new GPU array\n",
    "device_data = gpuarray.to_gpu(host_data)\n",
    "#perform pointwise multiplication on the GPU\n",
    "device_data_x2 = 2* device_data\n",
    "#retrieve the GPU data into a new with the gpuarray.get function\n",
    "host_data_x2 = device_data_x2.get()\n",
    "print(host_data_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU2DYoiMXP9J"
   },
   "source": [
    "## **Example: Doubling the value of elements in an array**\n",
    "\n",
    "Here, we will take an array and double the element of it on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHd1cmag_Wbx"
   },
   "source": [
    "### Step1: Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-g92eSBn5FlC"
   },
   "outputs": [],
   "source": [
    "# Declare the array as follows:\n",
    "np.random.seed(1729)\n",
    "a = np.random.randn(4,4).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AiqHKSh_ZdU"
   },
   "source": [
    "### Step 2: Transferring Data to the GPU\n",
    "\n",
    "The next step in most programs is to transfer data onto the device. In PyCuda, you will mostly transfer data from numpy arrays on the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92L9E7WkJhsy"
   },
   "outputs": [],
   "source": [
    "# firstly, we need to allocate memory on the device\n",
    "a_gpu = cuda.mem_alloc(a.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1xaAt_NJjSb"
   },
   "outputs": [],
   "source": [
    "# we need to transfer the data to the GPU\n",
    "cuda.memcpy_htod(a_gpu, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGxPkOSsARmP"
   },
   "source": [
    "### Step 3: Executing a Kernel\n",
    "\n",
    "For this tutorial, we will write code to double each entry in a_gpu. To this end, we write the corresponding CUDA C code, and feed it into the constructor of a [pycuda.compiler.SourceModule](https://documen.tician.de/pycuda/driver.html#pycuda.compiler.SourceModule):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSAkS6e2Jk38"
   },
   "outputs": [],
   "source": [
    "# Double each entry in the variable a_gpu\n",
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void twice(float *a)\n",
    "  {\n",
    "    int idx = threadIdx.x + threadIdx.y*4;\n",
    "    a[idx] *= 2;\n",
    "  }\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4NQf5XcAsEI"
   },
   "source": [
    "If there aren’t any errors, the code is now compiled and loaded onto the device. We find a reference to [pycuda.driver.Function](https://documen.tician.de/pycuda/driver.html#pycuda.driver.Function) and call it, specifying a_gpu as the argument, and a block size of 4x4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBQ0blkNJnIg"
   },
   "outputs": [],
   "source": [
    "func = mod.get_function(\"twice\")\n",
    "func(a_gpu, block=(4,4,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whSsByHBUaiY"
   },
   "source": [
    "Finally, we fetch the data back from the GPU and display it, together with the original a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ef82NDPJqWV"
   },
   "outputs": [],
   "source": [
    "a_doubled = np.empty_like(a)\n",
    "cuda.memcpy_dtoh(a_doubled, a_gpu)\n",
    "\n",
    "print(\"a\",a)\n",
    "print(\"\\nDoubled value\",a_doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHBgezKxBdnE"
   },
   "source": [
    "### Abstracting Away the Complications\n",
    "\n",
    "Using a [pycuda.gpuarray.GPUArray](https://documen.tician.de/pycuda/array.html#pycuda.gpuarray.GPUArray), the same effect can be achieved with much less writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24Wk5V33eLUQ"
   },
   "outputs": [],
   "source": [
    "# implementing with gpu_array\n",
    "a_gpu = gpuarray.to_gpu(a)\n",
    "a_doubled = (2*a_gpu).get()\n",
    "print(\"a\",a)\n",
    "print(\"\\nDoubled value\",a_doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Kd13_mbB8eb"
   },
   "source": [
    "### Shortcuts for Explicit Memory Copies\n",
    "\n",
    "The [pycuda.driver.In](https://documen.tician.de/pycuda/driver.html#pycuda.driver.In), [pycuda.driver.Out](https://documen.tician.de/pycuda/driver.html#pycuda.driver.Out), and [pycuda.driver.InOut](https://documen.tician.de/pycuda/driver.html#pycuda.driver.InOut) argument handlers can simplify some of the memory transfers. For example, instead of creating a_gpu, if replacing a is fine, the following code can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6-BYWwsniNR"
   },
   "outputs": [],
   "source": [
    "# implementing with InOut\n",
    "func(cuda.InOut(a), block=(4, 4, 1))\n",
    "print(\"a\",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhIg22xHYJU9"
   },
   "source": [
    "## **Example: Addition of two 1D arrays**\n",
    "\n",
    "Here, we will add two 1D arrays and execute it on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHZsceK_Zlxr"
   },
   "outputs": [],
   "source": [
    "# declare arrays\n",
    "m = np.random.randn(10).astype(np.float32)\n",
    "n = np.random.randn(10).astype(np.float32)\n",
    "\n",
    "# execute the kernel\n",
    "mod2_1D = SourceModule(\"\"\"\n",
    "__global__ void sum2arr(float *sum, float *m, float *n)\n",
    "{\n",
    "  const int i = threadIdx.x;\n",
    "  sum[i] = m[i] + n[i];\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "func = mod2_1D.get_function(\"sum2arr\")\n",
    "\n",
    "# handle memory transfer with 'Out()'\n",
    "sum_1D = np.zeros_like(m)\n",
    "func(cuda.Out(sum_1D),\n",
    "     cuda.In(m), cuda.In(n),\n",
    "     block=(10,1,1))\n",
    "\n",
    "# print result\n",
    "print(\"m\",m)\n",
    "print(\"\\nn\",n)\n",
    "print(\"\\nsum\",sum_1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Si7HbO0gbjTQ"
   },
   "source": [
    "## **Example: Addition of matrices**\n",
    "\n",
    "Here, we will add two matrices and execute it on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXx_p97mJs4Z"
   },
   "outputs": [],
   "source": [
    "# declare matrices\n",
    "b = np.random.randn(4,4).astype(np.float32)\n",
    "c = np.random.randn(4,4).astype(np.float32)\n",
    "\n",
    "# execute the kernel\n",
    "mod2_2D = SourceModule(\"\"\"\n",
    "  __global__ void add2(float *a, float *b)\n",
    "  {\n",
    "    int idx = threadIdx.x + threadIdx.y*4;\n",
    "    a[idx] += b[idx];\n",
    "  }\n",
    "  \"\"\")\n",
    "\n",
    "func = mod2_2D.get_function(\"add2\")\n",
    "\n",
    "# transfer the data to the GPU\n",
    "b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "c_gpu = cuda.mem_alloc(c.nbytes)\n",
    "\n",
    "cuda.memcpy_htod(b_gpu, b)\n",
    "cuda.memcpy_htod(c_gpu, c)\n",
    "\n",
    "func(b_gpu,c_gpu, block=(4,4,1))\n",
    "\n",
    "sum_2D = np.empty_like(b)\n",
    "cuda.memcpy_dtoh(sum_2D, b_gpu)\n",
    "\n",
    "# print result\n",
    "print(\"b\",b)\n",
    "print(\"\\nc\",c)\n",
    "print(\"\\nsum\",sum_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGGhkm8xY-cr"
   },
   "source": [
    "##**Example: Multiplication of matrices**\n",
    "\n",
    "Here, we will multiple two matrices and execute it on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dAKCn25Vggp"
   },
   "outputs": [],
   "source": [
    "# declare matrices\n",
    "r = np.random.randn(10).astype(np.float32)\n",
    "s = np.random.randn(10).astype(np.float32)\n",
    "\n",
    "# execute kernel\n",
    "mod3 = SourceModule(\"\"\"\n",
    "__global__ void multiply2arr(float *dest, float *r, float *s)\n",
    "{\n",
    "  const int i = threadIdx.x;\n",
    "  dest[i] = r[i] * s[i];\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "func = mod3.get_function(\"multiply2arr\")\n",
    "\n",
    "product = np.zeros_like(r)\n",
    "\n",
    "# handle memory transfer\n",
    "func(cuda.Out(product),\n",
    "     cuda.In(r), cuda.In(s),\n",
    "     block=(10,1,1))\n",
    "\n",
    "# print result\n",
    "print(\"r\",r)\n",
    "print(\"\\ns\",s)\n",
    "print(\"\\nproduct\",product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d69Myu1ksP-"
   },
   "source": [
    "## **Example: Linear combination of variables**\n",
    "\n",
    "The functionality in the module [pycuda.elementwise](https://documen.tician.de/pycuda/array.html#module-pycuda.elementwise) contains tools to help generate kernels that evaluate multi-stage expressions on one or several operands in a single pass. Here's a usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmqyWfJpjw7X"
   },
   "outputs": [],
   "source": [
    "# generate arrays of random numbers using curand()\n",
    "n1_gpu = curand((10,))\n",
    "n2_gpu = curand((10,))\n",
    "\n",
    "# generate a kernel that takes a number of scalar or vector arguments and performs the scalar operation on each entry of its arguments, if that argument is a vector.\n",
    "linear_combination = ElementwiseKernel(\n",
    "        \"float a, float *x, float b, float *y, float *z\",\n",
    "        \"z[i] = my_f(a*x[i], b*y[i])\",\n",
    "        \"linear_combination\",\n",
    "        preamble=\"\"\"\n",
    "        __device__ float my_f(float x, float y)\n",
    "        { \n",
    "          return sin(x*y);\n",
    "        }\n",
    "        \"\"\")\n",
    "\n",
    "# make a new, uninitialized GPUArray having the same properties as other_ary\n",
    "c_gpu = gpuarray.empty_like(n1_gpu)\n",
    "\n",
    "# call the function\n",
    "linear_combination(5, n1_gpu, 6, n2_gpu, c_gpu)\n",
    "\n",
    "# test for a particular condition\n",
    "assert la.norm(c_gpu.get() - np.sin((5*n1_gpu*6*n2_gpu).get())) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3y8KJLfTmTGo"
   },
   "source": [
    "## **Example: Numba and PyCUDA**\n",
    "\n",
    "The module [pycuda.autoprimaryctx](https://documen.tician.de/pycuda/util.html#module-pycuda.autoprimaryctx) is similar to [pycuda.autoinit](https://documen.tician.de/pycuda/util.html#module-pycuda.autoinit), except that it retains the device primary context instead of creating a new context in [pycuda.tools.make_default_context()](https://documen.tician.de/pycuda/util.html#pycuda.tools.make_default_context). Notably, it also has device and context attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDtaDlzzmWWv"
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "# Using autoprimaryctx instead of autoinit since Numba can only operate on a primary context\n",
    "import pycuda.autoprimaryctx  \n",
    "\n",
    "# Creating a PyCUDA gpuarray\n",
    "print(\"a\",a_gpu)\n",
    "\n",
    "# Numba kernel\n",
    "@cuda.jit\n",
    "def double(x):\n",
    "    i, j = cuda.grid(2)\n",
    "\n",
    "    if i < x.shape[0] and j < x.shape[1]:\n",
    "        x[i, j] *= 2\n",
    "\n",
    "# Calling the Numba kernel on the PyCUDA gpuarray, using the CUDA Array Interface transparently\n",
    "double[(4, 4), (1, 1)](a_gpu)\n",
    "print(\"Doubling values using numba: \",a_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjdE0VJJPNna"
   },
   "source": [
    "# **Exercise**\n",
    "\n",
    "- Write a cuda kernel to find the elementwise square of a matrix\n",
    "- Write a cuda kernel to find a matrix, which when added to the given matrix results in every element being equal to zero\n",
    "- Write a cuda kernel to multiply two matrices:\n",
    "  - Assume square matrices, with dimensions < 1024\n",
    "  - Assume square matrices, with dimensions > 1024\n",
    "  - Assume non-square matrices, with dimensions > 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B14RsBKfPO7v"
   },
   "outputs": [],
   "source": [
    "# Write your own code here"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Rakk0eUMHRR7",
    "u7uA7OVrTLIl",
    "cE8Wg9b84Zvi",
    "JU2DYoiMXP9J",
    "ZHd1cmag_Wbx",
    "0AiqHKSh_ZdU",
    "nGxPkOSsARmP",
    "UHBgezKxBdnE",
    "1Kd13_mbB8eb",
    "rhIg22xHYJU9",
    "Si7HbO0gbjTQ",
    "oGGhkm8xY-cr",
    "8d69Myu1ksP-",
    "3y8KJLfTmTGo",
    "AjdE0VJJPNna"
   ],
   "name": "Module-1-Lab-cuda-pycuda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
