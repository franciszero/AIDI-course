{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b6d4d9",
   "metadata": {},
   "source": [
    "### Python implementation of the iterative policy evaluation and update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "729b2522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sdhaka\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:159: UserWarning: pylab import has clobbered these variables: ['gamma', 'random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "%pylab inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f93115",
   "metadata": {},
   "source": [
    "Policy iteration\n",
    "Parameters\n",
    "ACTIONS = {\n",
    "    0: [1, 0],   # north\n",
    "    1: [-1, 0],  # south\n",
    "    2: [0, -1],  # west\n",
    "    3: [0, 1],   # east\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7e2679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1 # discounting rate\n",
    "rewardSize = -1\n",
    "gridSize = 4\n",
    "terminationStates = [[0,0], [gridSize-1, gridSize-1]]\n",
    "actions = [[-1, 0], [1, 0], [0, 1], [0, -1]]\n",
    "numIterations = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdaf5da",
   "metadata": {},
   "source": [
    "Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4397a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actionRewardFunction(initialPosition, action):\n",
    "    \n",
    "    if initialPosition in terminationStates:\n",
    "        return initialPosition, 0\n",
    "    \n",
    "    reward = rewardSize\n",
    "    finalPosition = np.array(initialPosition) + np.array(action)\n",
    "    if -1 in finalPosition or 4 in finalPosition: \n",
    "        finalPosition = initialPosition\n",
    "        \n",
    "    return finalPosition, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2ecfb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, -1], -1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actionRewardFunction([0,-1],[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41d25f",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66e01080",
   "metadata": {},
   "outputs": [],
   "source": [
    "valueMap = np.zeros((gridSize, gridSize))\n",
    "states = [[i, j] for i in range(gridSize) for j in range(gridSize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc4bcfe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# values of the value function at step 0\n",
    "valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0904153e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0],\n",
       " [0, 1],\n",
       " [0, 2],\n",
       " [0, 3],\n",
       " [1, 0],\n",
       " [1, 1],\n",
       " [1, 2],\n",
       " [1, 3],\n",
       " [2, 0],\n",
       " [2, 1],\n",
       " [2, 2],\n",
       " [2, 3],\n",
       " [3, 0],\n",
       " [3, 1],\n",
       " [3, 2],\n",
       " [3, 3]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a398f3",
   "metadata": {},
   "source": [
    "Policy evaluation\n",
    "delta =delta that reflect how much the value of a state changes respect the previous value. These deltas decay over the iterations and are supposed to reach 0 at the infinity.\n",
    "\n",
    "\n",
    "We iterate for each state and we calculate its new value as the weighted sum of the reward (-1) plus the value of each neighbor states (s’). Notice two things: the V(s’) is the expected value of the final/neighbor state s’ (at the beginning the expected value is 0 as we initialize the value function with zeroes). Finally, the V(s’) is multiplied by a gamma, which is the discounting factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "965aa3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "\n",
      "Iteration 2\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "\n",
      "Iteration 3\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "\n",
      "Iteration 10\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "\n",
      "Iteration 100\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "\n",
      "Iteration 1000\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltas = []\n",
    "for it in range(numIterations):\n",
    "    copyValueMap = np.copy(valueMap)\n",
    "    deltaState = []\n",
    "    for state in states:\n",
    "        weightedRewards = 0\n",
    "        for action in actions:\n",
    "            finalPosition, reward = actionRewardFunction(state, action)\n",
    "            weightedRewards += (1/len(actions))*(reward+(gamma*valueMap[finalPosition[0], finalPosition[1]]))\n",
    "            deltaState.append(np.abs(copyValueMap[state[0], state[1]]-weightedRewards))\n",
    "        copyValueMap[state[0], state[1]] = weightedRewards\n",
    "    deltas.append(deltaState)\n",
    "    valueMap = copyValueMap\n",
    "    if it in [0,1,2,9, 99, numIterations-1]:\n",
    "        print(\"Iteration {}\".format(it+1))\n",
    "        print(valueMap)\n",
    "        print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
