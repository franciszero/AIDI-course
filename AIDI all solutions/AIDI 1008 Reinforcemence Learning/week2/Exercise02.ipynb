{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAf-tEcWQJRm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_zlCkavQWJp",
    "outputId": "792dc8ff-c497-4393-e415-775379c3cb9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_states = 6\n",
    "\n",
    "# transition matrix together with policy\n",
    "\n",
    "P_pi = np.zeros((n_states, n_states))\n",
    "P_pi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XnJ95SLoofA",
    "outputId": "37305f04-b56b-4a60-f53e-21cc5a5a0775"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = np.zeros_like(P_pi)\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCa7WWpWRMj5"
   },
   "source": [
    "###Create the transition matrix by considering a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RgswIbxQhAt"
   },
   "outputs": [],
   "source": [
    "P_pi[0, 1] = 0.5\n",
    "\n",
    "P_pi[0, 3] = 0.5\n",
    "\n",
    "P_pi[1, 2] = 0.5\n",
    "\n",
    "P_pi[1, 5] = 0.5\n",
    "\n",
    "P_pi[2, 4] = 0.5\n",
    "\n",
    "P_pi[2, 5] = 0.5\n",
    "\n",
    "P_pi[4, 5] = 0.5\n",
    "\n",
    "P_pi[4, 0] = 0.5\n",
    "\n",
    "P_pi[3, 0] = 0.5\n",
    "\n",
    "P_pi[3, 3] = 0.5\n",
    "\n",
    "P_pi[5, 5] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WTWV3MUAQoj0",
    "outputId": "194aae03-b197-4dea-de66-dccbe80b18b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.5, 0. , 0.5, 0. , 0. ],\n",
       "       [0. , 0. , 0.5, 0. , 0. , 0.5],\n",
       "       [0. , 0. , 0. , 0. , 0.5, 0.5],\n",
       "       [0.5, 0. , 0. , 0.5, 0. , 0. ],\n",
       "       [0.5, 0. , 0. , 0. , 0. , 0.5],\n",
       "       [0. , 0. , 0. , 0. , 0. , 1. ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6YLTh1xQu_c"
   },
   "source": [
    "####Create the Reward Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGhxd8BdQy1_"
   },
   "outputs": [],
   "source": [
    "R[0, 1] = -2\n",
    "\n",
    "R[0, 3] = -1\n",
    "\n",
    "R[1, 2] = -2\n",
    "\n",
    "R[1, 5] = 0\n",
    "\n",
    "R[2, 4] = 15\n",
    "\n",
    "R[2, 5] = 10\n",
    "\n",
    "R[4, 5] = 10\n",
    "\n",
    "R[4, 0] = -10\n",
    "\n",
    "R[3, 3] = -1\n",
    "\n",
    "R[3, 0] = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qCVNksSNQ14E",
    "outputId": "50d0dd7c-d72d-4961-d61f-2e64508c6fed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,  -2.,   0.,  -1.,   0.,   0.],\n",
       "       [  0.,   0.,  -2.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,  15.,  10.],\n",
       "       [ -3.,   0.,   0.,  -1.,   0.,   0.],\n",
       "       [-10.,   0.,   0.,   0.,   0.,  10.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rzae_iClRY0e"
   },
   "source": [
    "###Being a probability matrix, the sum of all the columns of P_pi should be 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTW6mfbERaXk"
   },
   "outputs": [],
   "source": [
    "# check the correctness of P_pi\n",
    "\n",
    "assert((np.sum(P_pi, axis=1) == 1).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Fc1tYetRit5",
    "outputId": "9d7a8057-6817-4a5d-d0d3-870dfad27a9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5],\n",
       "       [-1. ],\n",
       "       [12.5],\n",
       "       [-2. ],\n",
       "       [ 0. ],\n",
       "       [ 0. ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expected reward for each state\n",
    "\n",
    "R_expected = np.sum(P_pi * R, axis=1, keepdims=True)\n",
    "\n",
    "R_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mOoyOCTRn_u"
   },
   "source": [
    "The R_expected vector contains the expected immediate reward for each state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X1QH7vgPRpvh",
    "outputId": "ee5c2b11-1a6c-4af8-b10f-ece5a3d3274d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.78587056],\n",
       "       [ 4.46226255],\n",
       "       [12.13836121],\n",
       "       [-5.09753046],\n",
       "       [-0.80364175],\n",
       "       [ 0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now it is possible to solve the Bellman Equation\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "A = np.eye(n_states, n_states) - gamma * P_pi\n",
    "\n",
    "B = R_expected\n",
    "\n",
    "# solve using scipy linalg\n",
    "\n",
    "V = linalg.solve(A, B)\n",
    "\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbXVMqSfR77d"
   },
   "source": [
    "This is the vector of the state values. State 0 has a value of -1.7, state 1 has a value of 4.4, and so on:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a5OO_quSJ4m"
   },
   "source": [
    "Let's examine how the results change with , which is the condition assumed for a myopic random student:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YK7xNqF0SK1O",
    "outputId": "1bc78051-f4ea-4d6a-cd53-5479804c3c2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5],\n",
       "       [-1. ],\n",
       "       [12.5],\n",
       "       [-2. ],\n",
       "       [ 0. ],\n",
       "       [ 0. ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.\n",
    "\n",
    "A = np.eye(n_states, n_states) - gamma * P_pi\n",
    "\n",
    "B = R_expected\n",
    "\n",
    "# solve using scipy linalg\n",
    "\n",
    "V_gamma_zero = linalg.solve(A, B)\n",
    "\n",
    "V_gamma_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZvd8QZDSZE-"
   },
   "source": [
    "As you can see, using , the value of each state is exactly equal to the expected immediate reward according to the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbQQ3za-TYYS"
   },
   "source": [
    "Now we can calculate the action-value function. We need to use a different form of immediate reward using a matrix with a shape of  formula |S .A,1|. Each row corresponds to a state-action pair, and the value is the immediate reward for that pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0kPTqjPTZ2G",
    "outputId": "030aee86-abf7-4a8d-be0c-a4348c688add"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_sa = np.zeros((n_states*2, 1))\n",
    "\n",
    "R_sa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAyMLkQ1ug_H"
   },
   "outputs": [],
   "source": [
    "R_sa[0] = -2 # study in state 0\n",
    "\n",
    "R_sa[1] = -1 # social in state 0\n",
    "\n",
    "R_sa[2] = -2 # study in state 1\n",
    "\n",
    "R_sa[3] = 0 # sleep in state 1\n",
    "\n",
    "R_sa[4] = 10 # sleep in state 2\n",
    "\n",
    "R_sa[5] = +15 # beer in state 2\n",
    "\n",
    "R_sa[6] = -1 # social in state 3 (social)\n",
    "\n",
    "R_sa[7] = -3 # study in state 3 (social)\n",
    "\n",
    "R_sa[8] = 10 # sleep in state 4 (pub)\n",
    "\n",
    "R_sa[9] = -10 # study in state 4 (pub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqvtqvtzTsBd",
    "outputId": "a114208b-efa2-4737-97f8-d752a02e0894"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_sa.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CstXot1T4ok"
   },
   "source": [
    "We now have to define the transition matrix of the student MDP. The transition matrix contains the probability of landing in a given state, starting from a state and an action. In the rows, we have the source state and action, and in the columns, we have the landing state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvT4y5GrT5_T",
    "outputId": "60e4818f-2108-491a-cb1f-7e277cfdfcd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transition Matrix (states x action, states)\n",
    "\n",
    "P = np.zeros((n_states*2, n_states))\n",
    "\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pcMTbqXv2Ls"
   },
   "outputs": [],
   "source": [
    "P[0, 1] = 1 # study in state 0 -> state 1\n",
    "\n",
    "P[1, 3] = 1 # social in state 0 -> state 3\n",
    "\n",
    "P[2, 2] = 1 # study in state 1 -> state 2\n",
    "\n",
    "P[3, 5] = 1 # sleep in state 1 -> state 5 (bed)\n",
    "\n",
    "P[4, 5] = 1 # sleep in state 2 -> state 5 (bed)\n",
    "\n",
    "P[5, 4] = 1 # beer in state 2 -> state 4 (pub)\n",
    "\n",
    "P[6, 3] = 1 # social in state 3 -> state 3 (social)\n",
    "P[7, 0] = 1 # study in state 3 -> state 0 (Class 1)\n",
    "\n",
    "P[8, 5] = 1 # sleep in state 4 -> state 5 (bed)\n",
    "\n",
    "P[9, 0] = 1 # study in state 4 -> state 0 (class 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSuvO5CoUQwF"
   },
   "source": [
    "We can now calculate the action-value function using gamma=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4h89V0g1URfX",
    "outputId": "70c19a70-0915-400e-a443-d139b6ef0f78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.01603629],\n",
       "       [ -5.58777741],\n",
       "       [  8.92452509],\n",
       "       [  0.        ],\n",
       "       [ 10.        ],\n",
       "       [ 14.27672242],\n",
       "       [ -5.58777741],\n",
       "       [ -4.60728351],\n",
       "       [ 10.        ],\n",
       "       [-11.60728351],\n",
       "       [  0.        ],\n",
       "       [  0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "Q_sa_pi = R_sa + gamma * P @ V\n",
    "\n",
    "Q_sa_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVZvugfZVJgE"
   },
   "source": [
    "The action-value vector contains the above values. Q_sa_pi is the action-value vector. For each state-action pair, we have the value of the action in that state.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FSdNPryVd02"
   },
   "source": [
    "We are now interested in extracting the best action for each state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRDsM9QgVfkP",
    "outputId": "53b3711c-e515-48ec-e5bc-ce0b5bc05533"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.01603629,  -5.58777741],\n",
       "       [  8.92452509,   0.        ],\n",
       "       [ 10.        ,  14.27672242],\n",
       "       [ -5.58777741,  -4.60728351],\n",
       "       [ 10.        , -11.60728351],\n",
       "       [  0.        ,   0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "reshape the column so that we obtain a vector with shape (n_states, n_actions)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "n_actions = 2\n",
    "\n",
    "Q_sa_pi2 = np.reshape(Q_sa_pi, (-1, n_actions))\n",
    "\n",
    "Q_sa_pi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWbJtqFkVMR7",
    "outputId": "8955d7b1-1036-4f47-e7fd-68c92c0e1ac3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In this way, performing the argmax function, we obtain the index of the best action in each state:\n",
    "\n",
    "best_actions = np.reshape(np.argmax(Q_sa_pi2, -1), (-1, 1))\n",
    "\n",
    "best_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8s_j3wUpYttv"
   },
   "source": [
    "The best_actions vector contains the above values."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
