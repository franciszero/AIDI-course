{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon greedy policy-MC On Policy\n",
    "\n",
    "\n",
    "the best known action based on our experience is selected with (1-epsilon) probability and the rest of time i.e. with epsilon probability any action is selected randomly.\n",
    "\n",
    "initially epsilon is 1 so we can explore more but as we do many iterations we slowly decrease the epsilon to 0 ( which is exploitation â†’ choosing the best known action)\n",
    "\n",
    "epsilon is between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from blackjack import BlackjackEnv\n",
    "\n",
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_epision_greedy_action_policy(Q,observation):\n",
    " \n",
    " Args:\n",
    " \n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "            \n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "            \n",
    "        nA: Number of actions in the environment.\n",
    "            \n",
    "        Returns:\n",
    "        \n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=0.1\n",
    "nA=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05, 0.05])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(nA, dtype=float) * epsilon / nA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>, {})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mc_control_epsilon_greedy(total_episodes):\n",
    "        \n",
    "      \n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        \n",
    "        total_episodes: Number of episodes to sample.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        policy is a function that takes an observation as an argument and returns action probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epision_greedy_action_policy(Q,observation):\n",
    "    \n",
    "    #Choose a random action with probability epsilon / nA\n",
    "    A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "    \n",
    "    # Get the action values corresponding to the observation(action_values = Q[observation]) & then Get the greedy/best action    \n",
    "    best_action = np.argmax(Q[observation])\n",
    "    \n",
    "    # Choose the greedy action with probability (1 - epsilon)   \n",
    "    A[best_action] += (1.0 - epsilon)\n",
    "    \n",
    "    #return the probability scores for each action\n",
    "    return A\n",
    "\n",
    "\n",
    "#This generates the episode by following the epsilon greedy policy\n",
    "def generate_episode(Q):\n",
    "    episode = []\n",
    "    current_state = env.reset()\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        # The optimal policy to be returned\n",
    "        prob_scores = get_epision_greedy_action_policy(Q,current_state)\n",
    "          \n",
    "         # sample the action from the epsilon greedy policy\n",
    "        action = np.random.choice(np.arange(len(prob_scores)), p=prob_scores) #0 or 1\n",
    "        \n",
    "         # perform the action in the environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode.append((current_state, action, reward))\n",
    "        if done:\n",
    "            break\n",
    "         # update the current state\n",
    "        current_state = next_state    \n",
    "        \n",
    "    return episode\n",
    "\n",
    "\n",
    "def mc_control_epsilon_greedy(total_episodes):\n",
    "  \n",
    "    returns_sum = defaultdict(float)\n",
    "    \n",
    "    ## store the number of times each state is visited \n",
    "    states_count = defaultdict(float)\n",
    "    \n",
    "    ## Action Value function to be returned \n",
    "    # where Number of actions = env.action_space.n\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    for k in range(total_episodes):\n",
    "        \n",
    "        episode = generate_episode(Q)\n",
    "        \n",
    "        state_actions_in_episode = list(set([(sar[0], sar[1]) for sar in episode]))\n",
    "        \n",
    "        for i,sa_pair in enumerate(state_actions_in_episode):\n",
    "            state, action = sa_pair\n",
    "    \n",
    "            G = sum([sar[2] for i,sar in enumerate(episode[i:])])\n",
    "            \n",
    "            #Taking the means of episodes to calculate mean values\n",
    "            returns_sum[sa_pair] += G\n",
    "            states_count[sa_pair] += 1.0\n",
    "            Q[state][action] = returns_sum[sa_pair] / states_count[sa_pair]\n",
    "        \n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output array contains the value function score for all the actions for each state ( here we have only two actions 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.mc_control_epsilon_greedy.<locals>.<lambda>()>,\n",
       "            {(17, 9, False): array([1., 0.])})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_control_epsilon_greedy(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action value function tells us how good is it to take that action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
