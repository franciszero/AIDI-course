{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c721362",
   "metadata": {},
   "source": [
    "# the first model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd845e4",
   "metadata": {},
   "source": [
    "### prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "af722486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/facebook/bart-large-mnli\n",
    "\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6cc44",
   "metadata": {},
   "source": [
    "### prepare the sequance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "06d33faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"one day I will see the world\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60fa75",
   "metadata": {},
   "source": [
    "### applying the classifier 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca4a1e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['life style', 'reality', 'revolution', 'science', 'politics'],\n",
       " 'scores': [0.45578545331954956,\n",
       "  0.27890530228614807,\n",
       "  0.1296178549528122,\n",
       "  0.08806750923395157,\n",
       "  0.04762383922934532]}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_labels = ['politics', 'science', 'life style', 'revolution', 'reality']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9024bee",
   "metadata": {},
   "source": [
    "### applying the classifier 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "655a3e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `multi_class` argument has been deprecated and renamed to `multi_label`. `multi_class` will be removed in a future version of Transformers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['travel', 'exploration', 'dancing', 'cooking'],\n",
       " 'scores': [0.994511067867279,\n",
       "  0.9383882284164429,\n",
       "  0.0057061817497015,\n",
       "  0.00181929103564471]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_labels = ['travel', 'cooking', 'dancing', 'exploration']\n",
    "classifier(sequence_to_classify, candidate_labels, multi_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c734185b",
   "metadata": {},
   "source": [
    "### the second model is a conversational model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6bf88fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: Hey! :D\n",
      ">> User:how are you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm good, how are you?\n",
      ">> User:i'm good too. it is a good day, yeah?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: It is indeed.\n",
      ">> User:glad to talk with you.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm glad to talk with you too.\n",
      ">> User:bye\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: Bye! :D\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/microsoft/DialoGPT-medium\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77505c46",
   "metadata": {},
   "source": [
    "# the third one is a Text2Text Generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c6f1abe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap\n",
    "# Tip: By now, install transformers from source\n",
    "\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "\n",
    "def get_question(answer, context, max_length=64):\n",
    "  input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n",
    "  features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "  output = model.generate(input_ids=features['input_ids'], \n",
    "               attention_mask=features['attention_mask'],\n",
    "               max_length=max_length)\n",
    "\n",
    "  return tokenizer.decode(output[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f410396e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> question: What element did the study reveal that seawater contained more phosphate than previously thought?</s>'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"\\\"This could really change how we think about the environments in which life first originated,\\\" \\\n",
    "said Professor Nick Tosca from the University of Cambridge, who was one of the authors of the study. \\\n",
    "The research, which was headed by University of Cambridge Ph.D. student Matthew Brady, \\\n",
    "reveals that early seawater may have carried 1,000–10,000 times more phosphate than previously thought, \\\n",
    "provided the water contained a lot of iron. Phosphate is a crucial component of DNA and RNA, \\\n",
    "which are the building blocks of life, \\\n",
    "although it is one of the least common elements in the universe relative to its biological significance. \\\n",
    "Phosphate is also relatively inaccessible in its mineral form – \\\n",
    "it can be difficult to dissolve in water so that life can utilize it.\"\n",
    "\n",
    "answer = \"iron\"\n",
    "\n",
    "get_question(answer, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc1f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3c321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e441642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e6d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
