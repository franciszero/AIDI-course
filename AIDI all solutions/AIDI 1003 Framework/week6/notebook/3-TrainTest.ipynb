{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting\n",
    "\n",
    "The reason for train/test splits of data is always, at heart, a desire to avoid overfitting.  It is straightforward in supervised learning problems to fit a model against all the available data.  Since we, by definition, do not yet have the data we do not have, we want a proxy for \"the novel data we might see in the future.\"\n",
    "\n",
    "Obviously, the best proxy we can come up with is simply a portion of the original data that did not participate in the fitting of the model.  We rely on an assumption that our sample data is similar to observations we will obtain in the future.  However, there is really nothing better we might choose as such a proxy.\n",
    "\n",
    "Using `train_test_split()` to divide the data between a training and testing set if a very reasonable approach.  By default, this utility function shuffles the data before splitting it; in general this will minimize effects related to order of collection or collation of the dataset.  However, especially on moderate sized datasets of hundreds or thousands of samples (but not really of tens of thousands, or millions), the particular accident of a randomized split can still lead to artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding splitting\n",
    "\n",
    "`train_test_split()` performs just one split of a data array, while all the other splitting classes in `sklearn.model_selection` produce an iterator over multiple distinct splits.  \n",
    "\n",
    "We will use the Iris dataset to illustrate these difference.  This dataset contains 150 observations of 3 different species of Iris, each sample containing 4 features.  It is a widely used example, and responds well to many classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to show some different behavior of splitting techniques, we will modify the Iris data to drop some of it.  In particular, we truncate the last 25 observations.  The reason we do this is because the samples in the dataset are grouped by their class, first all the Iris setosa, then all the Iris virginica, and finally all the Iris versicolor samples.  The truncation will create an imbalance among the classes of observations.  Most datasets you will encountere will have varying numbers of samples in different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris.data = iris.data[:-25]\n",
    "iris.target = iris.target[:-25]\n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic utility function—as we have seen in prior lessons—simply divides the data into two arrays.  We keep the same number of columns as in the original, but put some of the rows in the first array and the rest in the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(93, 4), (32, 4)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[arr.shape for arr in train_test_split(iris.data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If more than one array is passed to the function, it will split each one in turn; the split will be consistent in choosing the corresponding rows from each array (which must, therefore, all have a consistent number of rows).\n",
    "\n",
    "99% of the time, you use this behavior to simultaneously split features and target arrays.  In principle, the API of `train_test_split()` does nothing to enforce that usage.  In fact, in optionally taking more than two arguments, it is not constrained to this specific use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93, 4)\n",
      "(32, 4)\n",
      "(93,)\n",
      "(32,)\n",
      "(93, 3)\n",
      "(32, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for arr in train_test_split(iris.data, iris.target, np.ones((125,3))):\n",
    "    print(arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple splitting\n",
    "\n",
    "Classes for splitting create iterators over multiple splits using the same general algorithm for splitting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFold\n",
    "\n",
    "One of the simplest such techniques is `KFold`.  This simply divides the data into multiple \"folds.\"  By default, `KFold` does not shuffle the data first; therefore, if the dataset is meaningfully ordered in some manner already, the folds may have importantly different characteristics.  \n",
    "\n",
    "The potential differences among the folds can be good or bad, depending on your purpose.  Either way, be aware of it.  If you hope your model will generalize to sample collections with a characteristic not in the training, there is an advantage to not shuffling.  However, it equivalently means that a particular loop through fitting may not have the opportuntity to fit to data with that characteristic.\n",
    "\n",
    "<img src=\"KFold.png\" width=\"66%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we loop through a three-way split of the (truncated) Iris data.  The lengths of the the training versus testing data are slightly different between iterations simply because 125 is not divisible by 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0; Train shape: (83,); Test shape: (42,)\n",
      "Iteration: 1; Train shape: (83,); Test shape: (42,)\n",
      "Iteration: 2; Train shape: (84,); Test shape: (41,)\n"
     ]
    }
   ],
   "source": [
    "for n, (train, test) in enumerate(KFold(n_splits=3).split(iris.data)):\n",
    "    print(\"Iteration: %d; Train shape: %s; Test shape: %s\" % (\n",
    "                       n, train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that might be surprising at first is that the shape of training arrays are not, e.g. `(83, 4)`.  What we iterate over is a collections of index positions into the underlying NumPy arrays.  So, for example, in the first iteration, the test data is the first 1/3rd of the rows in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "37                4.9               3.6                1.4               0.1\n",
       "38                4.4               3.0                1.3               0.2\n",
       "39                5.1               3.4                1.5               0.2\n",
       "40                5.0               3.5                1.3               0.3\n",
       "41                4.5               2.3                1.3               0.3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = next(KFold(n_splits=3).split(iris.data))\n",
    "print(test)\n",
    "pd.DataFrame(iris.data[test], columns=iris.feature_names).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StratifiedKFold\n",
    "\n",
    "This cross-validation object is a variation of `KFold` that returns stratified folds. The folds are made by preserving the percentage of samples for each class.  Because this split is sensitive to the classes of the target, it must take a `y` argument to the `.split()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0; Train shape: (82,); Test shape: (43,)\n",
      "Iteration: 1; Train shape: (83,); Test shape: (42,)\n",
      "Iteration: 2; Train shape: (85,); Test shape: (40,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3).split(iris.data, iris.target)\n",
    "for n, (train, test) in enumerate(skf):\n",
    "    print(\"Iteration: %d; Train shape: %s; Test shape: %s\" % (\n",
    "                       n, train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the index positions generated for the first split are not successive from the head.  Rather there are 17 each from the first and second 50 samples, then 9 more from the last 25 samples.  Other folds are similar, with rounding producing slightly different counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  50\n",
      "  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66 100 101\n",
      " 102 103 104 105 106 107 108]\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=3).split(iris.data, iris.target)\n",
    "train, test = next(skf)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeaveOneOut\n",
    "\n",
    "This splitting technique utilizes the maximum possible size for each training set which still creating a nominal testing set.  This can be useful to train models as completely as possible while still allowing validation.  Of course, this iterates over a number of splits equal to the number of samples, so is the most computationally spendy split possible also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut().split(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0; Train shape: (124,); Test shape: (1,); Test index: [0]\n",
      "...\n",
      "Iteration: 124; Train shape: (124,); Test shape: (1,); Test index: [124]\n"
     ]
    }
   ],
   "source": [
    "all_folds = []\n",
    "for n, (train, test) in enumerate(loo):\n",
    "    all_folds.append((n, train, test))\n",
    "\n",
    "n, train, test = all_folds[0]\n",
    "print(\"Iteration: %d; Train shape: %s; Test shape: %s; Test index: %s\" % (\n",
    "    n, train.shape, test.shape, test))\n",
    "print(\"...\")\n",
    "n, train, test = all_folds[-1]\n",
    "print(\"Iteration: %d; Train shape: %s; Test shape: %s; Test index: %s\" % (\n",
    "    n, train.shape, test.shape, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupKFold\n",
    "\n",
    "A `KFold` variant with non-overlapping groups.  The same group will not appear in two different folds (the number of distinct groups has to be at least equal to the number of folds).\n",
    "\n",
    "The folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0; Train shape: (75,); Test shape: (50,)\n",
      "Iteration: 1; Train shape: (75,); Test shape: (50,)\n",
      "Iteration: 2; Train shape: (100,); Test shape: (25,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "gkf = GroupKFold(n_splits=3).split(iris.data, groups=iris.target)\n",
    "for n, (train, test) in enumerate(gkf):\n",
    "    print(\"Iteration: %d; Train shape: %s; Test shape: %s\" % (\n",
    "                       n, train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index postitions: [100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117\n",
      " 118 119 120 121 122 123 124]\n",
      "Species: ['virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# Verify that the final test group really is a homogeneous class\n",
    "print(\"Index postitions:\", test)\n",
    "print(\"Species:\", [iris.target_names[n] for n in iris.target[test]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "The splitters discussed in this lesson are only a few of those in scikit-learn.  A variety of others build on the general idea contained in those discussed.  Consult the [documentation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) for details on each.\n",
    "\n",
    "The point of all these splitters is almost universally to be used in conjunction with cross validation.  The function `cross_val_score()` performs repeated training and scoring relative to muliple train/test splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.  , 1.  , 1.  , 0.96, 1.  ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we mentioned, the Iris dataset is quite easy to fit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n",
    "scores                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an integer argument is given for `cv`, as above, it performs a Stratified KFold with that number of folds.  But we can also give an iterable like one of the scorers discussed.  \n",
    "\n",
    "The \"score\" given for each iteration is that produced by the `.score()` method of the estimator being used.  You can manually specify a different `scorer=my_scorer` parameter to `cross_val_score` if you want to use a different metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an artificially bad splitting strategy.  We train exclusively on two species on each iteration, then try to predict the excluded species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, iris.data, iris.target, \n",
    "                cv=GroupKFold(n_splits=3), groups=iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A less bad split for this particular dataset and classifier would be a basic KFold.  The data has an unequal number of samples from each class (by construction) and is orderd by class.  So this gets enough overlap to do well on some splits, but does poorly on others.\n",
    "\n",
    "In generally, Stratified KFold is pretty robust, and hence is the default strategy used by `cross_val_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.95238095, 0.3902439 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, iris.data, iris.target, cv=KFold(n_splits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean leave-one-out cross validation: 0.984\n",
      "All scores:\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "loo_cv = cross_val_score(clf, iris.data, iris.target, cv=LeaveOneOut())\n",
    "print(\"Mean leave-one-out cross validation:\", np.mean(loo_cv))\n",
    "print(\"All scores:\\n\", loo_cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
