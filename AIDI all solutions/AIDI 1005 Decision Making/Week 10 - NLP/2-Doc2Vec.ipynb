{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec: How To Implement doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Our Own Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data, clean it, and then split into train and test sets\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "messages = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
    "messages.columns = [\"label\", \"text\"]\n",
    "messages['text_clean'] = messages['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(messages['text_clean'],\n",
    "                                                    messages['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tagged document objects to prepare to train the model\n",
    "tagged_docs = [gensim.models.doc2vec.TaggedDocument(v, [i]) for i, v in enumerate(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['where', 'are', 'you', 'what', 'do', 'you', 'do', 'how', 'can', 'you', 'stand', 'to', 'be', 'away', 'from', 'me', 'doesn', 'your', 'heart', 'ache', 'without', 'me', 'don', 'you', 'wonder', 'of', 'me', 'don', 'you', 'crave', 'me'], tags=[0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at what a tagged document looks like\n",
    "tagged_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a basic doc2vec model\n",
    "d2v_model = gensim.models.Doc2Vec(tagged_docs,\n",
    "                                  vector_size=100,\n",
    "                                  window=5,\n",
    "                                  min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Parameter doc_words of infer_vector() must be a list of strings (not a single string).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-81bc935a6094>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# What happens if we pass in a single word like we did for word2vec?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0md2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36minfer_vector\u001b[1;34m(self, doc_words, alpha, min_alpha, epochs, steps)\u001b[0m\n\u001b[0;32m    660\u001b[0m         \"\"\"\n\u001b[0;32m    661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 662\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Parameter doc_words of infer_vector() must be a list of strings (not a single string).\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Parameter doc_words of infer_vector() must be a list of strings (not a single string)."
     ]
    }
   ],
   "source": [
    "# What happens if we pass in a single word like we did for word2vec?\n",
    "d2v_model.infer_vector('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.87602568e-03,  1.09385774e-02,  5.11323055e-03, -6.38195779e-03,\n",
       "       -7.25458469e-03, -1.13833696e-03, -6.78820815e-03, -1.70726702e-03,\n",
       "       -3.32039990e-03,  1.98232054e-04, -3.10476450e-03, -4.78714332e-03,\n",
       "        5.33253001e-03,  1.45982401e-02,  9.74501017e-03,  5.23984293e-03,\n",
       "        6.12392928e-03, -5.25344955e-03, -2.68952758e-03, -8.21829215e-03,\n",
       "       -8.90680961e-03, -7.21965916e-04, -3.64406733e-03, -8.28683563e-03,\n",
       "        7.45848985e-03, -4.47585061e-03, -1.33870905e-02, -3.34707974e-03,\n",
       "        6.89136097e-03,  1.11705114e-04,  2.12026876e-04, -4.58791066e-04,\n",
       "        1.59915863e-03, -1.15711205e-02, -2.57483544e-03,  1.36145744e-02,\n",
       "        1.09776948e-02,  5.33281709e-04,  3.53552634e-03,  1.56332254e-02,\n",
       "       -3.47791682e-03,  1.65766443e-03,  1.36225391e-02,  6.11415040e-03,\n",
       "       -6.41800230e-03,  7.22425990e-03,  2.14356207e-03, -2.71046138e-03,\n",
       "       -1.39552215e-03,  4.19839658e-03, -1.09695885e-02, -5.55958366e-03,\n",
       "        9.61300824e-03, -1.48416832e-02, -1.57901773e-03,  6.61182785e-05,\n",
       "        2.30384874e-03,  3.29921022e-04,  1.27628045e-02,  1.54009406e-02,\n",
       "        1.27532892e-02,  6.37671736e-04,  1.13933850e-02,  9.32398427e-04,\n",
       "       -2.66665663e-03,  6.50134450e-03, -1.97298615e-03, -1.28011331e-02,\n",
       "        7.10240519e-03,  5.65750990e-03, -1.44604845e-02, -6.18887274e-03,\n",
       "       -1.50147611e-02, -8.81731976e-05,  6.98689977e-03, -5.69416815e-03,\n",
       "        1.35349389e-02,  1.65890006e-03,  1.68832317e-02, -7.06236996e-03,\n",
       "        4.99697449e-03, -4.41357587e-03, -8.50789342e-03, -5.43382193e-04,\n",
       "        1.72130913e-02, -1.22986129e-02,  5.21491142e-03, -5.76004339e-03,\n",
       "       -6.57395087e-03,  9.49243363e-03,  2.26601516e-03,  7.66368932e-04,\n",
       "       -3.18856793e-03, -9.81137715e-03,  1.68585759e-02,  3.60474398e-04,\n",
       "       -1.85967865e-03,  5.00102993e-03,  2.64134677e-03,  1.08189890e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What happens if we pass in a list of words?\n",
    "d2v_model.infer_vector(['i', 'am', 'learning', 'nlp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What About Pre-trained Document Vectors?\n",
    "\n",
    "There are not as many options as there are for word vectors. There also is not an easy API to read these in like there is for `word2vec` so it is more time consuming.\n",
    "\n",
    "Pre-trained vectors from training on Wikipedia and Associated Press News can be found [here](https://github.com/jhlau/doc2vec). Feel free to explore on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec: How To Prep Document Vectors For Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00862826,  0.0071864 ,  0.00389735, -0.00425909, -0.00516394,\n",
       "       -0.0037196 , -0.00259269, -0.00301915, -0.0016474 , -0.00027165,\n",
       "       -0.00623732,  0.0017613 ,  0.00341308,  0.00484667,  0.00611341,\n",
       "        0.00290644,  0.00543516,  0.00146407,  0.00103653, -0.0077876 ,\n",
       "       -0.0061283 ,  0.00466484,  0.001793  , -0.00073053,  0.01112898,\n",
       "       -0.00975147, -0.00778145, -0.00735493,  0.00770985, -0.00276762,\n",
       "        0.00418301,  0.00120887,  0.0017338 , -0.00641951, -0.00638191,\n",
       "        0.00973248,  0.00934952, -0.00273729, -0.0006277 ,  0.00635287,\n",
       "       -0.0082921 ,  0.00341398,  0.00330042,  0.00868116, -0.00667403,\n",
       "        0.00203242,  0.00369902, -0.00717073, -0.00025978,  0.01032502,\n",
       "       -0.00880408, -0.00486287,  0.00879907, -0.00749715, -0.00053251,\n",
       "       -0.00608589,  0.00057463,  0.00435847,  0.01151353,  0.00404367,\n",
       "        0.00387285,  0.00121799,  0.0124625 , -0.00067046, -0.0008349 ,\n",
       "        0.01147593,  0.00045557, -0.00614358,  0.00686876, -0.00075762,\n",
       "       -0.01128279,  0.00208398, -0.00380641, -0.00563002,  0.00197907,\n",
       "       -0.00384768,  0.0128651 ,  0.00197249,  0.00751234, -0.00260865,\n",
       "       -0.00048992, -0.0024997 , -0.00947317,  0.00104961,  0.00475452,\n",
       "       -0.0083337 ,  0.00044215,  0.00214458, -0.00558318,  0.00701835,\n",
       "        0.00402741,  0.00243742, -0.00601373, -0.00907768,  0.01590819,\n",
       "       -0.0030302 ,  0.00037625, -0.00393223,  0.00272343,  0.0134301 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does a document vector look like again?\n",
    "d2v_model.infer_vector(['convert', 'words', 'to', 'vectors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we prepare these vectors to be used in a machine learning model?\n",
    "vectors = [[d2v_model.infer_vector(words)] for words in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-5.77358156e-03,  1.06707392e-02,  3.93922348e-03, -7.57708331e-04,\n",
       "        -2.58477381e-03, -1.92638807e-04, -4.11166903e-03,  1.42892252e-03,\n",
       "        -3.60492477e-03,  4.61327657e-03, -9.04983748e-03,  6.16300851e-04,\n",
       "         1.71596487e-03,  7.40852766e-03,  1.69022530e-02, -4.84264638e-06,\n",
       "         7.71544641e-03, -5.36660850e-03,  2.04246282e-03, -6.78323070e-03,\n",
       "        -6.98415888e-03, -6.06735048e-05,  2.13853503e-03, -3.96416103e-03,\n",
       "         5.87929925e-03, -6.58658426e-03, -5.63623197e-03, -6.96596922e-03,\n",
       "         1.16070481e-02,  3.42267728e-03,  2.08046148e-03,  5.73769910e-04,\n",
       "         7.51322089e-03, -5.91283245e-03, -6.49547437e-03,  4.98791132e-03,\n",
       "         1.54486941e-02, -6.46633422e-03, -5.08132624e-04,  1.61939599e-02,\n",
       "        -9.78285540e-03,  7.37955840e-03,  9.21900850e-03,  7.10050995e-03,\n",
       "        -3.65699898e-03,  6.58337492e-03,  9.00582317e-03, -2.08294671e-03,\n",
       "        -3.35605931e-03,  1.05053801e-02, -1.37188034e-02, -1.96189946e-03,\n",
       "         4.76055173e-03, -1.06228450e-02, -5.44930110e-03, -6.22786582e-03,\n",
       "         2.51025474e-03, -1.67453557e-03,  9.12162848e-03,  8.23699031e-03,\n",
       "         1.32255172e-02, -2.44494830e-03,  1.43300779e-02, -8.12552928e-04,\n",
       "        -2.04077270e-03,  7.58645870e-03,  1.88746664e-04, -8.42936244e-03,\n",
       "         1.16295265e-02,  1.47321308e-03, -1.23531939e-02,  1.27500261e-03,\n",
       "        -1.16170328e-02, -1.55584910e-03,  1.05702300e-02, -2.71312892e-03,\n",
       "         1.06562665e-02,  5.74449729e-03,  9.44897812e-03, -1.12905717e-02,\n",
       "         6.23272965e-03, -4.04302729e-03, -7.67718768e-03,  2.74470751e-03,\n",
       "         1.38001237e-02, -1.12124197e-02,  2.08319645e-04,  8.95940408e-04,\n",
       "         1.89939223e-03,  8.01222585e-03, -4.26463876e-03,  8.42036156e-04,\n",
       "        -1.51422434e-03, -1.17092235e-02,  1.71512254e-02, -2.83937994e-03,\n",
       "         2.49764998e-03,  1.05168286e-03,  4.17218497e-03,  1.43454364e-02],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
