{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <h1 align = \"center\"><div style = \"background-color: #C70000; color:white; border-radius: 15px; padding: 20px; margin: 2px;\">Red Wine Quality: XGBoost + Optuna</div></h1>\n\n<div style=\"width: 100%; text-align: center;\"> <img align = middle src=\"https://labelyourdata.com/img/article-illustrations/ml_essential_tool.jpg\" style = \"height: 650px;\" width = 100%></div>\n\n# <div style = \"background-color: #C70000; color:white; border-radius: 15px; padding: 20px; margin: 2px;\">Dataset Fields</div>\n> - **Fixed Acidity**: Fixed acidity corresponds to the set of low volatility organic acids such as malic, lactic, tartaric or citric acids and is inherent to the characteristics of the sample.\n> - **Volatile Acidity**: Volatile acidity is an important sensory parameter, with higher levels indicating wine spoilage.\n> - **Citric Acid**: Citric acid is often added to wines to increase acidity, complement a specific flavor or prevent ferric hazes. It can be added to finished wines to increase acidity and give a *fresh* flavor.\n> - **Residual Sugar**: Residual Sugar is from natural grape sugars leftover in a wine after the alcoholic fermentation finishes.\n> - **Chlorides**: They are a major contributor to the *saltiness* of a wine.\n> - **Free Sulfur Dioxide**: Sulfur dioxide (SO<sub>2</sub>) preserves wine, preventing oxidation and browning.\n> - **Total Sulfur Dioxide**: Total Sulfur Dioxide (TSO<sub>2</sub>) is the portion of SO<sub>2</sub> that is free in the wine plus the portion that is bound to other chemicals in the wine such as aldehydes, pigments, or sugars.\n> - **Density**: Holding alcohol level constant, density has little effect on the quality of wines as other keys can contribute in density.\n> - **pH**: Winemakers use pH as a way to measure ripeness in relation to acidity. Low pH wines will taste tart and crisp, while higher pH wines are more susceptible to bacterial growth.\n> - **Sulphates**: The presence of another type of sulpate, is thought to help rid the wine of a wide variety of bacteria (good and bad). This seems to lower the wine quality as well because it dulls the wine's fermentation process.\n> - **Alcohol**: Alcohol content affects a wine's body, since alcohol is more viscous than water. A wine with higher alcohol content will have a fuller, richer body, while a lower alcohol wine will taste lighter and more delicate on the palate.\n> - **Quality**: Wine quality mainly depends on the vinification process and the geographical origin of the grapes but also highly relies on the varietal composition of the grape.\n\n# <div style = \"background-color: #C70000; color:white; border-radius: 15px; padding: 20px; margin: 2px;\">Imports</div>\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.offsetbox import AnnotationBbox, OffsetImage\nimport matplotlib.image as mpimg\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n\nfrom catboost import CatBoostClassifier\n\nfrom xgboost import XGBClassifier\n\nimport lightgbm as lgbm\n\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\n\nimport optuna\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, Flatten, MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.utils import plot_model\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-09-03T07:24:23.751473Z","iopub.execute_input":"2022-09-03T07:24:23.751866Z","iopub.status.idle":"2022-09-03T07:24:32.630691Z","shell.execute_reply.started":"2022-09-03T07:24:23.751807Z","shell.execute_reply":"2022-09-03T07:24:32.629455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setting a color scheme","metadata":{}},{"cell_type":"code","source":"custom_colors = ['#A70000', '#CC0001', '#FF0000', '#FF5252', '#FF7B7B', '#FFBABA']\ncustom_palette = sns.set_palette(sns.color_palette(custom_colors))\nsns.palplot(sns.color_palette(custom_colors), size = 1)\nplt.tick_params(axis = 'both', labelsize = 0, length = 0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:24:32.633226Z","iopub.execute_input":"2022-09-03T07:24:32.634573Z","iopub.status.idle":"2022-09-03T07:24:32.930318Z","shell.execute_reply.started":"2022-09-03T07:24:32.634519Z","shell.execute_reply":"2022-09-03T07:24:32.92886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-03T07:24:32.935873Z","iopub.execute_input":"2022-09-03T07:24:32.938552Z","iopub.status.idle":"2022-09-03T07:24:33.009392Z","shell.execute_reply.started":"2022-09-03T07:24:32.938495Z","shell.execute_reply":"2022-09-03T07:24:33.008177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Taking a look at the missing values of the dataset","metadata":{}},{"cell_type":"code","source":"print(df.isna().sum())\nprint('================================')\nprint('Total Missing Values = {}'.format(df.isna().sum().sum()))\nprint('================================')","metadata":{"execution":{"iopub.status.busy":"2022-09-03T07:24:33.017196Z","iopub.execute_input":"2022-09-03T07:24:33.017988Z","iopub.status.idle":"2022-09-03T07:24:33.0364Z","shell.execute_reply.started":"2022-09-03T07:24:33.017936Z","shell.execute_reply":"2022-09-03T07:24:33.035312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Taking a look at the statistical summary of the dataset","metadata":{}},{"cell_type":"code","source":"summary = pd.DataFrame(df.describe())\nsummary = summary.style.background_gradient(cmap = 'Reds') \\\n          .set_table_attributes(\"style = 'display: inline'\") \\\n          .set_caption('Statistics of the Dataset') \\\n          .set_table_styles([{\n                'selector': 'caption',\n                'props': [\n                    ('font-size', '16px')\n                ]\n          }])\nsummary","metadata":{"execution":{"iopub.status.busy":"2022-09-03T07:24:33.040025Z","iopub.execute_input":"2022-09-03T07:24:33.040311Z","iopub.status.idle":"2022-09-03T07:24:33.220976Z","shell.execute_reply.started":"2022-09-03T07:24:33.040287Z","shell.execute_reply":"2022-09-03T07:24:33.219915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style = \"background-color: #C70000; color:white; border-radius: 15px; padding: 20px; margin: 2px;\">Exploratory Data Analysis (EDA)</div>\n","metadata":{}},{"cell_type":"code","source":"wine_glass = mpimg.imread('../input/plotimages/wawg.jpg')\nimagebox = OffsetImage(wine_glass, zoom = 0.5)\nxy = (0.5, 0.7)\nab = AnnotationBbox(imagebox, xy, frameon = False, pad = 1, xybox = (4.8, 525))\n\nplt.figure(figsize = (15, 10))\nax = sns.countplot(data = df, x = 'quality', palette = [custom_colors[0], custom_colors[1], custom_colors[2], custom_colors[3], custom_colors[4], custom_colors[5]])\nax.add_artist(ab)\n\nbbox_args = dict(boxstyle = 'round', fc = '0.9')\nfor p in ax.patches:\n        ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x() + 0.3, p.get_height() + 10.5),\n                   color = 'black',\n                   bbox = bbox_args,\n                   fontsize = 15)\n        \nplt.title('Quality Count for Red Wine', fontsize = 25)\nplt.xlabel('Quality', fontsize = 20)\nplt.ylabel('Count', fontsize = 20)\nplt.xticks(fontsize = 13)\nplt.yticks(fontsize = 13)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:24:33.22543Z","iopub.execute_input":"2022-09-03T07:24:33.228402Z","iopub.status.idle":"2022-09-03T07:24:33.667803Z","shell.execute_reply.started":"2022-09-03T07:24:33.228363Z","shell.execute_reply":"2022-09-03T07:24:33.666637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**: The count for wine with a quality of **5** is the highest and the count for wine with a quality of **3** is the lowest. From the countplot we can see that there is a **class imbalance** that exists in the dataset, and will need correction before we start training our ML models.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols = 2, figsize = (30, 10))\n\nsns.histplot(data = df, x = 'citric acid', kde = True, ax = axes[0])\naxes[0].set_title('Histogram for Citric Acid', fontsize = 25)\naxes[0].set_xlabel('Citric Acid', fontsize = 20)\naxes[0].set_ylabel('Count', fontsize = 20)\naxes[0].xaxis.set_tick_params(labelsize = 16)\naxes[0].yaxis.set_tick_params(labelsize = 16)\n\nsns.histplot(data = df, x = 'alcohol', kde = True, ax = axes[1])\naxes[1].set_title('Histogram for Alcohol', fontsize = 25)\naxes[1].set_xlabel('Alcohol', fontsize = 20)\naxes[1].set_ylabel('Count', fontsize = 20)\naxes[1].xaxis.set_tick_params(labelsize = 16)\naxes[1].yaxis.set_tick_params(labelsize = 16)\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:24:33.672889Z","iopub.execute_input":"2022-09-03T07:24:33.675605Z","iopub.status.idle":"2022-09-03T07:24:34.351062Z","shell.execute_reply.started":"2022-09-03T07:24:33.675566Z","shell.execute_reply":"2022-09-03T07:24:34.349835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**: The histograms for `citric acid` and `alcohol` are plotted to take a look at the skewness of their distributions. The histogram for **`alcohol`** exhibits **right skewness** where the `mode < median < mean`. \n\nBoxplots are a standardized way of displaying the distribution of data. A boxplot is a graph that gives you a good indication of how the values in the data are spread out. Although boxplots may seem primitive in comparison to a histogram or density plot, they have the advantage of taking up less space, which is useful when comparing distributions between many groups or datasets. This is what a typical boxplot would look like:\n\n<img src = \"https://phastdata.org/sites/phastdata.org/files/pictures/box_plot_anatomy.png\" width = 450>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (30, 30))\n\ndef create_boxplot(feature):\n    sns.boxplot(data = df, x = df['quality'], y = feature)\n    plt.title('Box Plot for ' + feature.title(), fontsize = 25)\n    plt.xlabel('Quality', fontsize = 20)\n    plt.ylabel(feature.title(), fontsize = 20)\n    plt.xticks(fontsize = 16)\n    plt.yticks(fontsize = 16)\n    \nplt.subplot(221)\ncreate_boxplot('fixed acidity')\n\nplt.subplot(222)\ncreate_boxplot('volatile acidity')\n\nplt.subplot(223)\ncreate_boxplot('citric acid')\n\nplt.subplot(224)\ncreate_boxplot('residual sugar')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:24:34.352363Z","iopub.execute_input":"2022-09-03T07:24:34.352657Z","iopub.status.idle":"2022-09-03T07:24:35.593228Z","shell.execute_reply.started":"2022-09-03T07:24:34.35263Z","shell.execute_reply":"2022-09-03T07:24:35.591703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n> - `Fixed Acidity`: The box plot for fixed acidity has approximately the same median for different qualities of wine. The outliers are the highest for wines with a quality of **5**.\n> - `Volatile Acidity`: As the quality of the wine increases we can observe that the median values of the volatile acidity of the wine **decreases**.\n> - `Citric Acid`: As the quality of the wine increases we can observe that the median values of the citric acid of the wine **increases**. This is exactly **opposite** to the observations we obtained while analysing the `volatile acidity` of the wine.\n> - `Residual Sugar`: The residual sugar has almost the **same** median values for different qualities of wine. Wine that has a quality of **5** or **6** has the highest number of outliers.\n\n\nA violin plot is a hybrid of a box plot and a kernel density plot, which shows peaks in the data. It is used to visualize the distribution of numerical data. Unlike a box plot that can only show summary statistics, violin plots depict summary statistics and the density of each variable. On each side of the gray line is a kernel density estimation to show the distribution shape of the data. Wider sections of the violin plot represent a higher probability that members of the population will take on the given value, the skinnier sections represent a lower probability. This is what a typical violin plot would look like:\n\n<img src = \"https://www.researchgate.net/profile/Jonathan-Chambers-3/publication/329035470/figure/fig15/AS:695026912870412@1542718737802/Explanation-of-Violin-plot-Densities-are-estimated-using-a-Gaussian-kernel-density.png\" width = 450>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (30, 30))\n\ndef create_violinplot(feature):\n    sns.violinplot(data = df, x = df['quality'], y = feature)\n    plt.title('Violin Plot for ' + feature.title(), fontsize = 25)\n    plt.xlabel('Quality', fontsize = 20)\n    plt.ylabel(feature.title(), fontsize = 20)\n    plt.xticks(fontsize = 16)\n    plt.yticks(fontsize = 16)\n    \nplt.subplot(221)\ncreate_violinplot('chlorides')\n\nplt.subplot(222)\ncreate_violinplot('free sulfur dioxide')\n\nplt.subplot(223)\ncreate_violinplot('total sulfur dioxide')\n\nplt.subplot(224)\ncreate_violinplot('density')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:24:35.59479Z","iopub.execute_input":"2022-09-03T07:24:35.595421Z","iopub.status.idle":"2022-09-03T07:24:37.175289Z","shell.execute_reply.started":"2022-09-03T07:24:35.595382Z","shell.execute_reply":"2022-09-03T07:24:37.174295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n> - `Chlorides`: The median values for chlorides is the **same** for different types of wine qualities.\n> - `Free Sulfur Dioxide`: The median value for free sulfur dioxide is the highest for wine with a quality of **5**.\n> - `Total Sulfur Dioxide`: The **highest IQR** for total sulpfur dioxide belongs to wine with a quality of 5.\n> - `Density`: Wine with a quality of 3 has the **highest median value** for density.\n\nThe style of **boxenplots** was originally named a `letter value` plot because it shows a large number of quantiles that are defined as `letter values`. It is similar to a box plot in plotting a nonparametric representation of a distribution in which all features correspond to actual observations. By plotting more quantiles, it provides more information about the shape of the distribution, particularly in the tails.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (30, 30))\n\ndef create_boxenplot(feature):\n    sns.boxenplot(data = df, x = df['quality'], y = feature)\n    plt.title('Boxenplot for ' + feature.title(), fontsize = 25)\n    plt.xlabel('Quality', fontsize = 20)\n    plt.ylabel(feature.title(), fontsize = 20)\n    plt.xticks(fontsize = 16)\n    plt.yticks(fontsize = 16)\n    \nplt.subplot(221)\ncreate_boxenplot('pH')\n\nplt.subplot(222)\ncreate_boxenplot('sulphates')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:24:37.181899Z","iopub.execute_input":"2022-09-03T07:24:37.182468Z","iopub.status.idle":"2022-09-03T07:24:37.880152Z","shell.execute_reply.started":"2022-09-03T07:24:37.182431Z","shell.execute_reply":"2022-09-03T07:24:37.879045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n> - `Ph`: The median values of the `ph` **decrease** as the quality of wine increases. Wines with a quality of 5 and 6 have longer heads and tails than the others.\n> - `Sulphates`: The median value of the `sulphates` **increases** with the increase in the quality of wine. Similar to the `ph`, wines with a quality of 5 and  have longer heads and tails than the rest.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (30, 30))\n\n\ndef create_scatterplot(feature1, feature2):\n    sns.scatterplot(data = df, x = feature1, y = feature2, hue = df['quality'], \n                    palette = [custom_colors[-1], custom_colors[-2], custom_colors[-3], \n                               custom_colors[-4], custom_colors[-5] ,custom_colors[-6]])\n    plt.title(feature1.title() + ' vs ' + feature2.title(), fontsize = 25)\n    plt.legend(fontsize = 15)\n    plt.xlabel(feature1.title(), fontsize = 20)\n    plt.ylabel(feature2.title(), fontsize = 20)\n    plt.xticks(fontsize = 16)\n    plt.yticks(fontsize = 16)\n    \nplt.subplot(221)\ncreate_scatterplot('pH', 'fixed acidity')\n\nplt.subplot(222)\ncreate_scatterplot('free sulfur dioxide', 'total sulfur dioxide')\n\nplt.subplot(223)\ncreate_scatterplot('alcohol', 'density')\n\nplt.subplot(224)\ncreate_scatterplot('chlorides', 'free sulfur dioxide')\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:24:37.884386Z","iopub.execute_input":"2022-09-03T07:24:37.886609Z","iopub.status.idle":"2022-09-03T07:24:40.367111Z","shell.execute_reply.started":"2022-09-03T07:24:37.886568Z","shell.execute_reply":"2022-09-03T07:24:40.365799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**: \n> - `pH` vs `fixed acidity`: The exists a **strong negative correlation** between the features of `pH` and `fixed acidity`.\n> - `free sulfur dioxide` vs `total sulfur dioxide`: There seems to be a **postive correlation** between these two features of the dataset.\n> - `alcohol` vs `density`: There exists a **negative correlation** between these two features.\n> - `chlorides vs free sulfur dioxide`: **No distinct correlation** can be observed from the scatterplot of these two features.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(data = df, hue = 'quality', palette = [custom_colors[0], custom_colors[1], custom_colors[2], \n                                                    custom_colors[3], custom_colors[4] ,custom_colors[5]])\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:24:40.368655Z","iopub.execute_input":"2022-09-03T07:24:40.369112Z","iopub.status.idle":"2022-09-03T07:25:25.11284Z","shell.execute_reply.started":"2022-09-03T07:24:40.369067Z","shell.execute_reply":"2022-09-03T07:25:25.111971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A **pairplot** plots a pairwise relationships in a dataset. The pairplot function creates a grid of Axes such that each variable in data will by shared in the Y-axis across a single row and in the X-axis across a single column.\n\n# <div style = \"background-color: #C70000; color:white; border-radius: 15px; padding: 20px; margin: 2px;\">Checking for Correlation</div>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (20, 20))\nsns.heatmap(df.corr(), cmap = 'Reds', square = True, annot = True, annot_kws = {'size': 16},\n           cbar_kws = {'shrink': 0.80})\nplt.title(\"Visualizing Correlations\", size = 25)\nplt.xticks(fontsize = 16)\nplt.yticks(fontsize = 16)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:25:25.114284Z","iopub.execute_input":"2022-09-03T07:25:25.114845Z","iopub.status.idle":"2022-09-03T07:25:26.011931Z","shell.execute_reply.started":"2022-09-03T07:25:25.114794Z","shell.execute_reply":"2022-09-03T07:25:26.010888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To correct the **class imbalance** in the dataset, we make use of binning. We divide the dataset into `bad` and `good` quality wine, such that the number of samples for **bad** quality wine is almost equal to the number of samples for good quality wine.","metadata":{}},{"cell_type":"code","source":"df['quality'] = pd.cut(df['quality'], bins = [1, 5, 10], labels = ['bad', 'good'])\n\nplt.figure(figsize = (15, 10))\nax = sns.countplot(df['quality'], palette = [custom_colors[1], custom_colors[-3]])\nbbox_args = dict(boxstyle = 'round', fc = '0.9')\nfor p in ax.patches:\n        ax.annotate('{:.0f} = {:.2f}%'.format(p.get_height(), (p.get_height() / len(df['quality'])) * 100), (p.get_x() + 0.3, p.get_height() + 13), \n                   color = 'black',\n                   bbox = bbox_args,\n                   fontsize = 15)\n\nplt.title('Quality Count for Red Wine', fontsize = 25)\nplt.xlabel('Quality', fontsize = 20)\nplt.ylabel('Count', fontsize = 20)\nplt.xticks(fontsize = 13)\nplt.yticks(fontsize = 13)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-03T07:25:26.013705Z","iopub.execute_input":"2022-09-03T07:25:26.014088Z","iopub.status.idle":"2022-09-03T07:25:26.238752Z","shell.execute_reply.started":"2022-09-03T07:25:26.014049Z","shell.execute_reply":"2022-09-03T07:25:26.237803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style = \"background-color: #C70000; color:white; border-radius: 15px; padding: 20px; margin: 2px;\">Encoding the Labels</div>","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ndf['quality'] = label_encoder.fit_transform(df['quality'])\ndf['quality'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-09-03T07:25:26.240276Z","iopub.execute_input":"2022-09-03T07:25:26.240899Z","iopub.status.idle":"2022-09-03T07:25:26.251973Z","shell.execute_reply.started":"2022-09-03T07:25:26.240858Z","shell.execute_reply":"2022-09-03T07:25:26.250963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style = \"background-color: #C70000; color:white; border-radius: 15px; padding: 20px; margin: 2px;\">Scaling the Data</div>\nStandardScaler standardizes a feature by subtracting the mean and then scaling it to unit variance.\n\n<img src = \"https://miro.medium.com/max/971/1*Nlgc_wq2b-VfdawWX9MLWA.png\" width = 250>","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nfeatures = [features for features in df.columns if df[features].dtype != int]\ndf[features] = scaler.fit_transform(df[features])\ndf","metadata":{"execution":{"iopub.status.busy":"2022-09-03T07:25:26.254886Z","iopub.execute_input":"2022-09-03T07:25:26.255143Z","iopub.status.idle":"2022-09-03T07:25:26.282496Z","shell.execute_reply.started":"2022-09-03T07:25:26.255118Z","shell.execute_reply":"2022-09-03T07:25:26.281691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop('quality', axis = 1)\ny = df['quality']\nprint(X, '\\n\\n\\n', y)","metadata":{"execution":{"iopub.status.busy":"2022-09-03T07:25:26.284518Z","iopub.execute_input":"2022-09-03T07:25:26.285347Z","iopub.status.idle":"2022-09-03T07:25:26.299967Z","shell.execute_reply.started":"2022-09-03T07:25:26.285312Z","shell.execute_reply":"2022-09-03T07:25:26.298999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style = \"background-color: #C70000; color:white; border-radius: 15px; padding: 20px; margin: 2px;\">Train-Test Split</div>","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)","metadata":{"execution":{"iopub.status.busy":"2022-09-03T07:25:26.301508Z","iopub.execute_input":"2022-09-03T07:25:26.301883Z","iopub.status.idle":"2022-09-03T07:25:26.311504Z","shell.execute_reply.started":"2022-09-03T07:25:26.301844Z","shell.execute_reply":"2022-09-03T07:25:26.310595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style = \"background-color: #C70000; color:white; border-radius: 15px; padding: 20px; margin: 2px;\">Training ML Models</div>","metadata":{}},{"cell_type":"code","source":"algo_name = []\naccuracy = []\n\ndef display_results_and_graphs(algorithm_name, model): \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    acc_model = model.score(X_test, y_test)\n    \n    algo_name.append(algorithm_name)\n    accuracy.append(acc_model)\n    \n    print(f'======For {algorithm_name}======')\n    print('Training Accuracy: {}%\\nTesting Accuracy: {}%\\nF1 Score: {}'.\n          format((model.score(X_train, y_train) * 100), \n                 model.score(X_test, y_test) * 100, \n                 f1_score(y_test, y_pred)))\n    print('\\n')\n    \n    fig, axes = plt.subplots(1, 2, figsize = (15, 8))\n    \n    fig.suptitle('Graphs for ' + algorithm_name, fontsize = 25)\n    \n    sns.heatmap(confusion_matrix(y_test, y_pred), annot = True, \n                cmap = 'Reds', annot_kws = {'size': 15}, \n                square = True, fmt = '.0f',\n                ax = axes[0])\n    axes[0].set_title('Confusion Matrix', fontsize = 20)\n    \n    fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n    roc_auc = metrics.auc(fpr, tpr)\n    sns.lineplot(fpr, tpr, ax = axes[1], color = 'red')\n    axes[1].set_title('ROC Curve (' + str(round(roc_auc, 3)) + ')', fontsize = 20)\n    axes[1].plot([0, 1], [0, 1,], 'b--')\n    plt.show()\n    \ndisplay_results_and_graphs('Logistic Regression', LogisticRegression())\ndisplay_results_and_graphs('K Nearest Neighbors', KNeighborsClassifier(n_neighbors = 13))\ndisplay_results_and_graphs('Support Vector Classifier', SVC())\ndisplay_results_and_graphs('Decision Tree Classifier', DecisionTreeClassifier(random_state = 0))\ndisplay_results_and_graphs('Random Forest Classifer', RandomForestClassifier(random_state = 0))\ndisplay_results_and_graphs('Gradient Boosting Classifier', GradientBoostingClassifier(random_state = 0))\ndisplay_results_and_graphs('Ada Boost Classifier', AdaBoostClassifier(random_state = 0))\ndisplay_results_and_graphs('Cat Boost Classifier', CatBoostClassifier(verbose = 0))\ndisplay_results_and_graphs('XGBoost Classifier', XGBClassifier())\ndisplay_results_and_graphs('Light Gradient Boosting Machine', lgbm.LGBMClassifier())","metadata":{"execution":{"iopub.status.busy":"2022-09-03T07:25:26.313306Z","iopub.execute_input":"2022-09-03T07:25:26.313803Z","iopub.status.idle":"2022-09-03T07:25:35.79848Z","shell.execute_reply.started":"2022-09-03T07:25:26.313769Z","shell.execute_reply":"2022-09-03T07:25:35.796891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_comparison = {}\n\nfor k, v in zip(algo_name, accuracy):\n    model_comparison.update({k: v * 100})\n\nmodel_comparison = dict(sorted(model_comparison.items(), key = lambda x: x[1], reverse = True))\nmodels = list(model_comparison.keys())\naccuracy = list(model_comparison.values())\n\nplt.figure(figsize = (15, 10))\nsns.barplot(x = accuracy, y = models)\nplt.title('Model Comparison', fontsize = 25)\nplt.xlabel('Accuracy', fontsize = 20)\nplt.ylabel('Models Used', fontsize = 20)\nplt.xticks(size = 15)\nplt.yticks(size = 15)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:25:35.800219Z","iopub.execute_input":"2022-09-03T07:25:35.8013Z","iopub.status.idle":"2022-09-03T07:25:36.114615Z","shell.execute_reply.started":"2022-09-03T07:25:35.801255Z","shell.execute_reply":"2022-09-03T07:25:36.113602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations** We see that ***Random Forest*** gives us the best results with an accuracy of 82.8125%. ***LGBM*** and ***XGBoost*** provide the second and third best results respectively. To improve the accuracy we'll be performing hyperparameter optimization using **Optuna**.\n\n# <div style = \"background-color: #C70000; color:white; border-radius: 15px; padding: 20px; margin: 2px;\">Hyperparameter Optimization Using Optuna</div>\n\n<img src = \"https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png\" width = 350>\n\n**Optuna** is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, ***define-by-run*** style user API. The code written with Optuna enjoys high modularity, allowing the user to dynamically construct search spaces for the hyperparameters.\n\n***Note***: Even though Random Forest gave the best default accuracy, we'll be optimizing the hyperparamters of XGBoost, as it gives better results than an optimized version of Random Forest.","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    param_grid = {\n        'tree_method': 'gpu_hist', \n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4, 0.5, 0.6, 0.7, 0.8, 1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008, 0.01, 0.012, 0.014, 0.016, 0.018, 0.02]),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, 50),\n        'max_depth': trial.suggest_categorical('max_depth', [5, 7, 9, 11, 13, 15, 17]),\n        'random_state': 0,\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n    }\n    \n    model = XGBClassifier(**param_grid)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    acc_model = model.score(X_test, y_test)\n    \n    return acc_model\n\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 300)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-09-03T07:25:36.117986Z","iopub.execute_input":"2022-09-03T07:25:36.118632Z","iopub.status.idle":"2022-09-03T07:36:44.391526Z","shell.execute_reply.started":"2022-09-03T07:25:36.118591Z","shell.execute_reply":"2022-09-03T07:36:44.390804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of finished trials:', len(study.trials))\nprint('Best Parameters:', study.best_trial.params)\nprint('Improvement in XGBClassifier Accuracy: {}%'.format((study.best_trial.value * 100) - model_comparison['XGBoost Classifier']))","metadata":{"execution":{"iopub.status.busy":"2022-09-03T07:36:44.394874Z","iopub.execute_input":"2022-09-03T07:36:44.395881Z","iopub.status.idle":"2022-09-03T07:36:44.455045Z","shell.execute_reply.started":"2022-09-03T07:36:44.395809Z","shell.execute_reply":"2022-09-03T07:36:44.453231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style = \"background-color: #C70000; color:white; border-radius: 15px; padding: 20px; margin: 2px;\">Visualizing the Results of Hyperparameter Optimization</div>","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:36:44.456296Z","iopub.execute_input":"2022-09-03T07:36:44.456673Z","iopub.status.idle":"2022-09-03T07:36:44.628505Z","shell.execute_reply.started":"2022-09-03T07:36:44.456632Z","shell.execute_reply":"2022-09-03T07:36:44.627454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_parallel_coordinate(study)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:36:44.629864Z","iopub.execute_input":"2022-09-03T07:36:44.630307Z","iopub.status.idle":"2022-09-03T07:36:44.778943Z","shell.execute_reply.started":"2022-09-03T07:36:44.63027Z","shell.execute_reply":"2022-09-03T07:36:44.777878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_slice(study)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:36:44.78024Z","iopub.execute_input":"2022-09-03T07:36:44.781288Z","iopub.status.idle":"2022-09-03T07:36:45.15331Z","shell.execute_reply.started":"2022-09-03T07:36:44.781249Z","shell.execute_reply":"2022-09-03T07:36:45.15239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_contour(study,\n                                  params = ['alpha',\n                                           'learning_rate',\n                                           'max_depth',\n                                           'n_estimators']\n                                 )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:36:45.154666Z","iopub.execute_input":"2022-09-03T07:36:45.15561Z","iopub.status.idle":"2022-09-03T07:36:48.152666Z","shell.execute_reply.started":"2022-09-03T07:36:45.155573Z","shell.execute_reply":"2022-09-03T07:36:48.151604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:36:48.154284Z","iopub.execute_input":"2022-09-03T07:36:48.154623Z","iopub.status.idle":"2022-09-03T07:37:02.29274Z","shell.execute_reply.started":"2022-09-03T07:36:48.154592Z","shell.execute_reply":"2022-09-03T07:37:02.291531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_edf(study)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:37:02.299217Z","iopub.execute_input":"2022-09-03T07:37:02.299547Z","iopub.status.idle":"2022-09-03T07:37:02.318873Z","shell.execute_reply.started":"2022-09-03T07:37:02.299511Z","shell.execute_reply":"2022-09-03T07:37:02.317777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.drop(columns = ['free sulfur dioxide'], axis = 1)\ntemp = np.array(X_train)\nX_train_nn = temp.reshape(-1, 2, 5, 1)\nprint('New shape of training data:', X_train_nn.shape)\n\nX_test = X_test.drop(columns = ['free sulfur dioxide'], axis = 1)\ntemp = np.array(X_test)\nX_test_nn = temp.reshape(-1, 2, 5, 1)\nprint('New shape of testing data:', X_test_nn.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-03T07:37:02.320627Z","iopub.execute_input":"2022-09-03T07:37:02.321176Z","iopub.status.idle":"2022-09-03T07:37:02.332555Z","shell.execute_reply.started":"2022-09-03T07:37:02.321125Z","shell.execute_reply":"2022-09-03T07:37:02.331287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style = \"background-color: #C70000; color:white; border-radius: 15px; padding: 20px; margin: 2px;\">Prediction Using Neural Networks</div>","metadata":{}},{"cell_type":"code","source":"model = Sequential([\n    Input(shape = (2, 5, 1)),\n    Conv2D(32, 3, padding = 'same', activation = 'relu'),\n    Conv2D(32, 3, padding = 'same', activation = 'relu'),\n    Conv2D(32, 3, padding = 'same', activation = 'relu'),\n    MaxPooling2D(),\n    Conv2D(64, 3, padding = 'same', activation = 'relu'),\n    Conv2D(64, 3, padding = 'same', activation = 'relu'),\n    Conv2D(64, 3, padding = 'same', activation = 'relu'),\n    Flatten(),\n    Dropout(0.2),\n    Dense(256, input_shape = (2, 5, 1), activation = 'relu'),\n    Dense(128, activation = 'relu'),\n    Dense(32, activation = 'relu'),\n    Dense(1, activation = 'sigmoid')\n])\n\nmodel.compile(loss = 'binary_crossentropy',\n              optimizer = Adam(),\n              metrics = ['acc'])\n\nreduce_lr = ReduceLROnPlateau(monitor = 'acc', patience = 3, verbose = 1, factor = 0.5, min_lr = 0.00001)","metadata":{"execution":{"iopub.status.busy":"2022-09-03T07:37:02.334633Z","iopub.execute_input":"2022-09-03T07:37:02.335517Z","iopub.status.idle":"2022-09-03T07:37:07.050158Z","shell.execute_reply.started":"2022-09-03T07:37:02.33546Z","shell.execute_reply":"2022-09-03T07:37:07.049084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-09-03T07:37:07.051647Z","iopub.execute_input":"2022-09-03T07:37:07.052413Z","iopub.status.idle":"2022-09-03T07:37:07.060886Z","shell.execute_reply.started":"2022-09-03T07:37:07.052374Z","shell.execute_reply":"2022-09-03T07:37:07.059625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, show_shapes = True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:37:07.062861Z","iopub.execute_input":"2022-09-03T07:37:07.063295Z","iopub.status.idle":"2022-09-03T07:37:08.279879Z","shell.execute_reply.started":"2022-09-03T07:37:07.063251Z","shell.execute_reply":"2022-09-03T07:37:08.278753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train_nn, y_train,\n                    epochs = 100,\n                    callbacks = [reduce_lr])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-09-03T07:37:08.281804Z","iopub.execute_input":"2022-09-03T07:37:08.282124Z","iopub.status.idle":"2022-09-03T07:37:34.793715Z","shell.execute_reply.started":"2022-09-03T07:37:08.282095Z","shell.execute_reply":"2022-09-03T07:37:34.792761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_performance_graphs(classifier):\n    \n    fig, axes = plt.subplots(1, 2, figsize = (15, 8))\n\n    axes[0].plot(classifier.epoch, classifier.history['acc'], label = 'acc')\n    axes[0].set_title('Accuracy vs Epochs', fontsize = 20)\n    axes[0].set_xlabel('Epochs', fontsize = 15)\n    axes[0].set_ylabel('Accuracy', fontsize = 15)\n    axes[0].legend()\n\n    axes[1].plot(classifier.epoch, classifier.history['loss'], label = 'loss')\n    axes[1].set_title(\"Loss Curve\",fontsize=18)\n    axes[1].set_xlabel(\"Epochs\",fontsize=15)\n    axes[1].set_ylabel(\"Loss\",fontsize=15)\n    axes[1].legend()\n\n    plt.show()\n    \nmodel_performance_graphs(history)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:37:34.79544Z","iopub.execute_input":"2022-09-03T07:37:34.795806Z","iopub.status.idle":"2022-09-03T07:37:35.117781Z","shell.execute_reply.started":"2022-09-03T07:37:34.795767Z","shell.execute_reply":"2022-09-03T07:37:35.116808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_train_acc = model.evaluate(X_train_nn, y_train)[-1]\nnn_test_acc = model.evaluate(X_test_nn, y_test)[-1]\nprint(nn_train_acc, nn_test_acc)","metadata":{"execution":{"iopub.status.busy":"2022-09-03T07:37:35.119443Z","iopub.execute_input":"2022-09-03T07:37:35.12011Z","iopub.status.idle":"2022-09-03T07:37:35.64934Z","shell.execute_reply.started":"2022-09-03T07:37:35.120071Z","shell.execute_reply":"2022-09-03T07:37:35.648411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = (model.predict(X_test_nn) > 0.5).astype(int)\n\nprint('======For the Neural Network======')\nprint('Training Accuracy: {}%\\nTesting Accuracy: {}%\\nF1-Score: {}'.format(nn_train_acc * 100, nn_test_acc * 100, f1_score(y_test, y_pred)))\n\nfig, axes = plt.subplots(1, 2, figsize = (15, 8))\n    \nfig.suptitle('Graphs for the Neural Network', fontsize = 25)\n\nsns.heatmap(confusion_matrix(y_test, y_pred), annot = True, \n            cmap = 'Reds', annot_kws = {'size': 15}, \n            square = True, fmt = '.0f',\n            ax = axes[0])\naxes[0].set_title('Confusion Matrix', fontsize = 20)\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\nsns.lineplot(fpr, tpr, ax = axes[1], color = 'red')\naxes[1].set_title('ROC Curve (' + str(round(roc_auc, 3)) + ')', fontsize = 20)\naxes[1].plot([0, 1], [0, 1,], 'b--')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-03T07:37:35.651684Z","iopub.execute_input":"2022-09-03T07:37:35.652741Z","iopub.status.idle":"2022-09-03T07:37:36.187853Z","shell.execute_reply.started":"2022-09-03T07:37:35.652703Z","shell.execute_reply":"2022-09-03T07:37:36.18674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style = \"background-color: #C70000; color:white; border-radius: 15px; padding: 20px; margin: 2px;\">References</div>\n> - https://www.kaggle.com/code/vishalyo990/prediction-of-quality-of-wine\n> - https://www.kaggle.com/code/madhurisivalenka/basic-machine-learning-with-red-wine-quality-data\n> - https://www.kaggle.com/code/nkitgupta/feature-engineering-and-feature-selection\n> - https://www.analyticsvidhya.com/blog/2020/11/hyperparameter-tuning-using-optuna/","metadata":{}}]}