{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMR5HDyAu1J7"
   },
   "source": [
    "Lab - 6 Question : Select any of the datset discussed in today's lecture and implement following algorithms \n",
    "* DT, RF, LR, NB\n",
    "* Compare the performance and comment on which algorithm will be most suitable for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import category_encoders as ce\n",
    "from time import time\n",
    "from xgboost import *\n",
    "from scipy.stats import normaltest\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.cluster import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.datasets import load_digits, make_hastie_10_2\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# %matplotlib\n",
    "# %matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.width', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pl5hpBAuueck"
   },
   "outputs": [],
   "source": [
    "class Foo:\n",
    "    def __init__(self):\n",
    "        data = './adult_NB.csv'\n",
    "        self.df = pd.read_csv(data, header=None, names=np.array(\n",
    "            ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation',\n",
    "             'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "             'income']))\n",
    "        # self.df['workclass'].replace('?', np.NaN, inplace=True)\n",
    "        # self.df['occupation'].replace('?', np.NaN, inplace=True)\n",
    "        # self.df['native_country'].replace('?', np.NaN, inplace=True)\n",
    "        self.X, self.y = None, None\n",
    "\n",
    "    def get_dfx(self):\n",
    "        if self.X is None:\n",
    "            self.X = self.df.drop(['income'], axis=1)\n",
    "        return self.X\n",
    "\n",
    "    def get_dfy(self):\n",
    "        if self.y is None:\n",
    "            self.y = self.df['income']\n",
    "        return self.y\n",
    "\n",
    "    def check_category(self):\n",
    "        for _, col in enumerate(self.df.columns):\n",
    "            print(\"Column Name:\", col)\n",
    "            print(\"Categorical display:\")\n",
    "            print(pd.value_counts(self.df[col]))\n",
    "            print('-' * 100)\n",
    "\n",
    "    def check_null(self):\n",
    "        print(self.df.isnull().values.any())\n",
    "        plt.figure(figsize=(8, 3), dpi=100)\n",
    "        sns.heatmap(self.df.isnull(), cmap=\"viridis\")\n",
    "\n",
    "    def __label_encoding(self, c):\n",
    "        self.df[c] = LabelEncoder().fit_transform(self.df[c])\n",
    "        pass\n",
    "\n",
    "    def label_encoding(self):\n",
    "        # print(\"label encoding:\")\n",
    "        # print(self.df.head())\n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].dtype == 'O':\n",
    "                self.__label_encoding(col)\n",
    "                # print(\"label encoding:%s\\n%s\" % (col, self.df.head()))\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def scaling(f, c, action=None, plot=False):\n",
    "        tmp = f[[c]]\n",
    "        if action == 'log':\n",
    "            tmp = np.log(tmp + 1)\n",
    "            color = 'r'\n",
    "        elif action == 'minmax':\n",
    "            tmp = MinMaxScaler().fit_transform(tmp)\n",
    "            color = 'g'\n",
    "        elif action == 'standard':\n",
    "            tmp = StandardScaler().fit_transform(tmp)\n",
    "            color = 'b'\n",
    "        else:\n",
    "            color = 'k'\n",
    "\n",
    "        if plot:\n",
    "            plt.figure(figsize=(7, 2), dpi=70)\n",
    "            sns.histplot(tmp, kde=True, color=color)\n",
    "            plt.title(\"\\\"%s\\\" scaling:%s, (%.2f, %.2f)\" % (c, action, tmp.min(), tmp.max()))\n",
    "            plt.show()\n",
    "        f[c] = tmp\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def search_cv(model, params, fold, verbose=0):\n",
    "        return RandomizedSearchCV(model, params, cv=fold, verbose=verbose, return_train_score=True, )\n",
    "        # return RandomizedSearchCV(model, params, cv=fold, verbose=0,\n",
    "        #                           scoring=scoring, refit=list(scoring.items())[0][0], return_train_score=True, )\n",
    "\n",
    "#LogisticRegression\n",
    "    def train_LR(self, x, y, random_state=0, verbose=0):\n",
    "        model = LogisticRegression()\n",
    "        params = {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 1e4]\n",
    "        }\n",
    "        k_fold = StratifiedKFold(5, shuffle=True, random_state=random_state)\n",
    "        clf = self.search_cv(model, params, k_fold, verbose=verbose)\n",
    "        clf.fit(x, y)\n",
    "        return \"LR\", clf\n",
    "\n",
    "    #DecisionTreeClassifier\n",
    "    def train_DT(self, x, y, random_state=0, verbose=0):\n",
    "        model = DecisionTreeClassifier()\n",
    "        params = {\n",
    "            'max_depth': [2, 3, 5, 10, 20],\n",
    "            'min_samples_leaf': [5, 10, 20, 50, 100],\n",
    "            'criterion': [\"gini\", \"entropy\"]\n",
    "        }\n",
    "        k_fold = StratifiedKFold(5, shuffle=True, random_state=random_state)\n",
    "        clf = self.search_cv(model, params, k_fold, verbose=verbose)\n",
    "        clf.fit(x, y)\n",
    "        return \"DT\", clf\n",
    "\n",
    "    #GaussianNB\n",
    "    def train_NB(self, x, y, random_state=0, verbose=0):\n",
    "        model = GaussianNB()\n",
    "        params = {\n",
    "            'var_smoothing': [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15]\n",
    "        }\n",
    "        k_fold = StratifiedKFold(5, shuffle=True, random_state=random_state)\n",
    "        clf = self.search_cv(model, params, k_fold, verbose=verbose)\n",
    "        clf.fit(x, y)\n",
    "        return \"NB\", clf\n",
    "\n",
    "    #RandomForestClassifier\n",
    "    def train_RF(self, x, y, random_state=0, verbose=0):\n",
    "        model = RandomForestClassifier()\n",
    "        params = {\n",
    "            'bootstrap': [True, False],\n",
    "            'max_depth': [2, 4, 8, 16, 32],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'n_estimators': [10, 20, 50, 100, 200],\n",
    "            'criterion': ['gini', 'entropy']\n",
    "        }\n",
    "        # scoring = {\n",
    "        #     \"Accuracy\": make_scorer(accuracy_score),\n",
    "        #     \"mean_absolute_error\": make_scorer(mean_absolute_error),\n",
    "        #     \"mean_squared_error\": make_scorer(mean_squared_error),\n",
    "        #     \"r2_score\": make_scorer(r2_score),\n",
    "        # }\n",
    "        k_fold = StratifiedKFold(5, shuffle=True, random_state=random_state)\n",
    "        clf = self.search_cv(model, params, k_fold, verbose=verbose)\n",
    "        clf.fit(x, y)\n",
    "        return \"RF\", clf\n",
    "\n",
    "    def onehot_encoding(self, x_train, x_test):\n",
    "        X = self.get_dfx()\n",
    "        categorical_data = [var for var in X.columns if X[var].dtype == 'O']\n",
    "\n",
    "        encoder = ce.OneHotEncoder(cols=categorical_data)\n",
    "        x_train = encoder.fit_transform(x_train)\n",
    "        x_test = encoder.transform(x_test)\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        x_train = scaler.fit_transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the performance\n",
    "class Driver:\n",
    "    def __init__(self, testing=True, frac=0.01, test_split_size=0.1):\n",
    "        self.foo = Foo()\n",
    "        if testing:\n",
    "            self.foo.label_encoding()\n",
    "            self.foo.df = self.foo.df.sample(frac=frac, replace=False, random_state=0)\n",
    "        r = random.randint(1, 10000)\n",
    "        print(\"test site split random_state : %d\" % r)\n",
    "        self.X = self.foo.get_dfx()\n",
    "        self.y = self.foo.get_dfy()\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = \\\n",
    "            train_test_split(self.X, self.y, test_size=test_split_size, random_state=r, stratify=self.y)\n",
    "        if not testing:\n",
    "            self.X_train, self.X_test = self.foo.onehot_encoding(self.X_train, self.X_test)\n",
    "        print(\"sample size: %s\" % str(self.foo.df.shape))\n",
    "        print(\"training data size: %s\" % str(self.X_train.shape))\n",
    "    pass\n",
    "\n",
    "    #comment on which algorithm will be most suitable for the dataset\n",
    "    def model_select(self, results):\n",
    "        print(\"Now the best model is\")\n",
    "        best_model_name = None\n",
    "        best_accuracy = 0.0\n",
    "        self.__print_title()\n",
    "        for _, tup in enumerate(results):\n",
    "            self.__print_result(tup)\n",
    "            name = tup[0]\n",
    "            accuracy = tup[1][2]\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model_name = name\n",
    "        print(\"%s is the most suitable as it's accuracy score is %.4f which is the highest of the four algorithms.\\n\" % (best_model_name.split(' ')[0], best_accuracy))\n",
    "\n",
    "    def evaluating(self, callable_training, verbose=0):\n",
    "        self.__print_title()\n",
    "        buffer = []\n",
    "        name = None\n",
    "        for _ in range(5):\n",
    "            name, clf = callable_training(self.X_train, self.y_train,\n",
    "                                          random_state=random.randint(1, 10000), verbose=verbose)\n",
    "            start = time()\n",
    "            y_pred = clf.predict(self.X_test)\n",
    "            cost = (time() - start) * 1000\n",
    "            # tmp_params = clf.cv_results_['params'][clf.best_index_]\n",
    "            tup = (\n",
    "                round(clf.cv_results_['mean_test_score'][clf.best_index_], 3),\n",
    "                round(clf.cv_results_['std_test_score'][clf.best_index_] * 2, 3),\n",
    "                round(accuracy_score(self.y_test, y_pred), 3),\n",
    "                round(precision_score(self.y_test, y_pred, average='macro'), 3),\n",
    "                round(recall_score(self.y_test, y_pred, average='macro'), 3),\n",
    "                round(cost, 3),\n",
    "            )\n",
    "            buffer.append(np.array(tup))\n",
    "            self.__print_result((name, tup))\n",
    "        tup = np.array(buffer).mean(axis=0)\n",
    "        self.__print_result((\"%s avg\" % name, tup))\n",
    "        print()\n",
    "        return \"%s avg\" % name, tup\n",
    "\n",
    "    @staticmethod\n",
    "    def __print_title():\n",
    "        print(\"%15s\\t%10s\\t%10s\\t%10s\\t%10s\\t%10s\\t%10s\" % (\n",
    "            \"model\", \"mean\", \"std\", \"accuracy\", \"precision\", \"recall\", \"latency\"))\n",
    "\n",
    "    @staticmethod\n",
    "    def __print_result(tup):\n",
    "        name, r = tup\n",
    "        if name.find(\"avg\") >= 0:\n",
    "            std = \"\"\n",
    "        else:\n",
    "            std = \"+/-%.3f\" % r[1]\n",
    "        print(\"%15s\\t%10s\\t%10s\\t%10s\\t%10s\\t%10s\\t%10s\" % (\n",
    "            \"%s\" % name, \"%.3f\" % r[0], std, \"%.3f\" % r[2], \"%.3f\" % r[3], \"%.3f\" % r[4], \"%.3f ms\" % r[5]))\n",
    "\n",
    "    def evaluating_model(self, clf):\n",
    "        pred = clf.predict(self.X_test)\n",
    "        print('Training set score: {:.4f}'.format(clf.score(self.X_train, self.y_train)))\n",
    "        print('Test set score: {:.4f}'.format(clf.score(self.X_test, self.y_test)))\n",
    "        print('Model accuracy score: {0:0.4f}'.format(accuracy_score(self.y_test, pred)))\n",
    "        print(classification_report(self.y_test, pred))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test site split random_state : 7272\n",
      "sample size: (32561, 15)\n",
      "training data size: (29304, 108)\n"
     ]
    }
   ],
   "source": [
    "go = Driver(testing=False, test_split_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          model\t      mean\t       std\t  accuracy\t precision\t    recall\t   latency\n",
      "             RF\t     0.863\t  +/-0.006\t     0.850\t     0.820\t     0.743\t105.672 ms\n",
      "             RF\t     0.862\t  +/-0.010\t     0.848\t     0.807\t     0.751\t 15.110 ms\n",
      "             RF\t     0.864\t  +/-0.007\t     0.850\t     0.809\t     0.759\t 12.882 ms\n",
      "             RF\t     0.866\t  +/-0.003\t     0.853\t     0.814\t     0.762\t 43.260 ms\n",
      "             RF\t     0.864\t  +/-0.006\t     0.849\t     0.817\t     0.740\t 60.794 ms\n",
      "         RF avg\t     0.864\t          \t     0.850\t     0.813\t     0.751\t 47.544 ms\n",
      "\n",
      "          model\t      mean\t       std\t  accuracy\t precision\t    recall\t   latency\n",
      "             LR\t     0.847\t  +/-0.006\t     0.836\t     0.785\t     0.738\t  0.537 ms\n",
      "             LR\t     0.847\t  +/-0.012\t     0.839\t     0.790\t     0.744\t  0.517 ms\n",
      "             LR\t     0.848\t  +/-0.012\t     0.838\t     0.787\t     0.747\t  0.513 ms\n",
      "             LR\t     0.847\t  +/-0.006\t     0.838\t     0.788\t     0.743\t  0.485 ms\n",
      "             LR\t     0.848\t  +/-0.008\t     0.836\t     0.785\t     0.738\t  0.526 ms\n",
      "         LR avg\t     0.847\t          \t     0.837\t     0.787\t     0.742\t  0.516 ms\n",
      "\n",
      "          model\t      mean\t       std\t  accuracy\t precision\t    recall\t   latency\n",
      "             DT\t     0.855\t  +/-0.004\t     0.849\t     0.818\t     0.740\t  1.332 ms\n",
      "             DT\t     0.855\t  +/-0.008\t     0.844\t     0.816\t     0.724\t  1.385 ms\n",
      "             DT\t     0.857\t  +/-0.006\t     0.846\t     0.815\t     0.733\t  1.318 ms\n",
      "             DT\t     0.854\t  +/-0.009\t     0.840\t     0.798\t     0.731\t  1.439 ms\n",
      "             DT\t     0.852\t  +/-0.006\t     0.841\t     0.808\t     0.724\t  1.216 ms\n",
      "         DT avg\t     0.855\t          \t     0.844\t     0.811\t     0.730\t  1.338 ms\n",
      "\n",
      "          model\t      mean\t       std\t  accuracy\t precision\t    recall\t   latency\n",
      "             NB\t     0.801\t  +/-0.011\t     0.789\t     0.731\t     0.787\t  5.137 ms\n",
      "             NB\t     0.796\t  +/-0.004\t     0.790\t     0.722\t     0.623\t  5.544 ms\n",
      "             NB\t     0.801\t  +/-0.008\t     0.789\t     0.731\t     0.787\t  5.232 ms\n",
      "             NB\t     0.795\t  +/-0.009\t     0.787\t     0.715\t     0.621\t  5.426 ms\n",
      "             NB\t     0.800\t  +/-0.008\t     0.789\t     0.731\t     0.787\t  6.677 ms\n",
      "         NB avg\t     0.799\t          \t     0.789\t     0.726\t     0.721\t  5.603 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arr = [\n",
    "    go.evaluating(go.foo.train_RF, verbose=0),\n",
    "    go.evaluating(go.foo.train_LR, verbose=0),\n",
    "    go.evaluating(go.foo.train_DT, verbose=0),\n",
    "    go.evaluating(go.foo.train_NB, verbose=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the best model is\n",
      "          model\t      mean\t       std\t  accuracy\t precision\t    recall\t   latency\n",
      "         RF avg\t     0.864\t          \t     0.850\t     0.813\t     0.751\t 47.544 ms\n",
      "         LR avg\t     0.847\t          \t     0.837\t     0.787\t     0.742\t  0.516 ms\n",
      "         DT avg\t     0.855\t          \t     0.844\t     0.811\t     0.730\t  1.338 ms\n",
      "         NB avg\t     0.799\t          \t     0.789\t     0.726\t     0.721\t  5.603 ms\n",
      "RF is the most suitable as it's accuracy score is 0.8500 which is the highest of the four algorithms.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "go.model_select(arr)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AIDI_1002_Week_5_Lecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
